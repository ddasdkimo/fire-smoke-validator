#!/usr/bin/env python3
"""
Gradio è³‡æ–™æ¨™è¨˜ä»‹é¢
ç”¨æ–¼æ¨™è¨˜ç«ç½åµæ¸¬æ¨¡å‹çš„è¼¸å‡ºçµæœï¼Œå€åˆ†çœŸå¯¦ç«ç½èˆ‡èª¤åˆ¤
"""

import gradio as gr
import cv2
import numpy as np
from pathlib import Path
import json
import shutil
from datetime import datetime
import os
import tempfile
from collections import defaultdict

try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False
    print("âš ï¸  Ultralytics æœªå®‰è£ï¼Œå°‡ä½¿ç”¨æ¨¡æ“¬åµæ¸¬")

class DataLabelingTool:
    def __init__(self, model_path="best.pt", data_dir="data/raw/HPWREN/extracted"):
        self.data_dir = Path(data_dir)
        self.model_path = model_path
        self.output_dir = Path("data/labeled")
        self.output_dir.mkdir(exist_ok=True)
        
        # å»ºç«‹åˆ†é¡è³‡æ–™å¤¾
        self.categories = {
            "true_fire": self.output_dir / "true_fire",      # çœŸå¯¦ç«ç½
            "true_smoke": self.output_dir / "true_smoke",    # çœŸå¯¦ç…™éœ§
            "false_cloud": self.output_dir / "false_cloud",  # é›²æœµèª¤åˆ¤
            "false_light": self.output_dir / "false_light",  # ç‡ˆå…‰èª¤åˆ¤
            "false_static": self.output_dir / "false_static", # éœæ…‹ç‰©é«”èª¤åˆ¤
            "uncertain": self.output_dir / "uncertain"       # ä¸ç¢ºå®š
        }
        
        for category_path in self.categories.values():
            category_path.mkdir(exist_ok=True)
        
        # è¼‰å…¥æ¨¡å‹
        self.model = None
        if ULTRALYTICS_AVAILABLE and Path(model_path).exists():
            try:
                self.model = YOLO(model_path)
                # åœ¨ Mac ä¸Šä½¿ç”¨ MPS è¨­å‚™
                import torch
                if torch.backends.mps.is_available():
                    self.model.to('mps')
                    print(f"âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹: {model_path} (ä½¿ç”¨ MPS åŠ é€Ÿ)")
                else:
                    print(f"âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹: {model_path} (ä½¿ç”¨ CPU)")
            except Exception as e:
                print(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
        
        # æ¨™è¨˜æ­·å²
        self.labeling_history = []
        self.current_detections = {}
        
    def get_mp4_files(self):
        """ç²å–æ‰€æœ‰ MP4 æª”æ¡ˆåˆ—è¡¨"""
        mp4_files = []
        for mp4_path in self.data_dir.rglob("*.mp4"):
            relative_path = mp4_path.relative_to(self.data_dir)
            mp4_files.append(str(relative_path))
        return sorted(mp4_files)
    
    def process_video(self, video_path):
        """è™•ç†å½±ç‰‡ä¸¦é€²è¡Œç«ç½åµæ¸¬"""
        if not video_path:
            return "è«‹é¸æ“‡ä¸€å€‹å½±ç‰‡æª”æ¡ˆ", None, []
        
        full_video_path = self.data_dir / video_path
        if not full_video_path.exists():
            return f"æª”æ¡ˆä¸å­˜åœ¨: {video_path}", None, []
        
        try:
            # è®€å–å½±ç‰‡
            cap = cv2.VideoCapture(str(full_video_path))
            if not cap.isOpened():
                return f"ç„¡æ³•é–‹å•Ÿå½±ç‰‡: {video_path}", None, []
            
            # ç²å–å½±ç‰‡è³‡è¨Š
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = frame_count / fps if fps > 0 else 0
            
            # é€²è¡Œåµæ¸¬ï¼ˆæ¯éš”å¹¾å¹€è™•ç†ä¸€æ¬¡ä»¥ç¯€çœæ™‚é–“ï¼‰
            detections = []
            frame_idx = 0
            sample_interval = max(1, fps // 2)  # æ¯ 0.5 ç§’æ¡æ¨£ä¸€æ¬¡
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                if frame_idx % sample_interval == 0:
                    # é€²è¡Œç«ç½åµæ¸¬
                    detection_result = self.detect_fire_smoke(frame, frame_idx, fps)
                    if detection_result['has_detection']:
                        detections.append(detection_result)
                
                frame_idx += 1
            
            cap.release()
            
            # å„²å­˜åµæ¸¬çµæœ
            self.current_detections[video_path] = detections
            
            info = f"å½±ç‰‡è³‡è¨Šï¼š\n"
            info += f"- æª”æ¡ˆ: {video_path}\n"
            info += f"- æ™‚é•·: {duration:.1f} ç§’\n"
            info += f"- å¹€ç‡: {fps} FPS\n"
            info += f"- ç¸½å¹€æ•¸: {frame_count}\n"
            info += f"- åµæ¸¬åˆ°çš„ç›®æ¨™: {len(detections)} å€‹\n"
            
            # å»ºç«‹åµæ¸¬çµæœçš„é¸é …
            detection_options = []
            for i, det in enumerate(detections):
                time_str = f"{det['timestamp']:.1f}s"
                conf_str = f"{det['confidence']:.2f}"
                class_name = det['class_name']
                detection_options.append(f"[{i+1}] {time_str} - {class_name} ({conf_str})")
            
            return info, str(full_video_path), detection_options
            
        except Exception as e:
            return f"è™•ç†å½±ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", None, []
    
    def detect_fire_smoke(self, frame, frame_idx, fps):
        """å°å–®ä¸€å¹€é€²è¡Œç«ç½ç…™éœ§åµæ¸¬"""
        timestamp = frame_idx / fps
        
        if self.model is not None:
            try:
                # ä½¿ç”¨ YOLO æ¨¡å‹é€²è¡Œåµæ¸¬
                results = self.model(frame, verbose=False)
                
                for result in results:
                    if result.boxes is not None and len(result.boxes) > 0:
                        # å–ç¬¬ä¸€å€‹åµæ¸¬çµæœ
                        box = result.boxes[0]
                        confidence = float(box.conf[0])
                        class_id = int(box.cls[0])
                        class_name = self.model.names[class_id]
                        
                        # å–å¾—é‚Šç•Œæ¡†åº§æ¨™
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        
                        return {
                            'has_detection': True,
                            'timestamp': timestamp,
                            'frame_idx': frame_idx,
                            'confidence': confidence,
                            'class_name': class_name,
                            'class_id': class_id,
                            'bbox': [int(x1), int(y1), int(x2), int(y2)],
                            'frame': frame.copy()
                        }
                
            except Exception as e:
                print(f"åµæ¸¬æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        # æ¨¡æ“¬åµæ¸¬çµæœï¼ˆå¦‚æœæ²’æœ‰æ¨¡å‹ï¼‰
        if np.random.random() < 0.1:  # 10% æ©Ÿç‡ç”¢ç”Ÿåµæ¸¬
            class_names = ['fire', 'smoke']
            class_name = np.random.choice(class_names)
            h, w = frame.shape[:2]
            
            return {
                'has_detection': True,
                'timestamp': timestamp,
                'frame_idx': frame_idx,
                'confidence': np.random.uniform(0.5, 0.95),
                'class_name': class_name,
                'class_id': 0 if class_name == 'fire' else 1,
                'bbox': [
                    np.random.randint(0, w//2),
                    np.random.randint(0, h//2),
                    np.random.randint(w//2, w),
                    np.random.randint(h//2, h)
                ],
                'frame': frame.copy()
            }
        
        return {'has_detection': False}
    
    def show_detection(self, video_path, detection_idx):
        """é¡¯ç¤ºç‰¹å®šçš„åµæ¸¬çµæœ"""
        if not video_path or video_path not in self.current_detections:
            return None, "è«‹å…ˆè™•ç†å½±ç‰‡"
        
        detections = self.current_detections[video_path]
        if not detections or detection_idx < 0 or detection_idx >= len(detections):
            return None, "åµæ¸¬ç´¢å¼•ç„¡æ•ˆ"
        
        detection = detections[detection_idx]
        frame = detection['frame']
        bbox = detection['bbox']
        
        # åœ¨å¹€ä¸Šç¹ªè£½é‚Šç•Œæ¡†
        annotated_frame = frame.copy()
        x1, y1, x2, y2 = bbox
        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        # æ·»åŠ æ¨™ç±¤
        label = f"{detection['class_name']} {detection['confidence']:.2f}"
        cv2.putText(annotated_frame, label, (x1, y1-10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # è½‰æ›ç‚º RGB
        annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
        
        info = f"æ™‚é–“: {detection['timestamp']:.1f}s\n"
        info += f"é¡åˆ¥: {detection['class_name']}\n"
        info += f"ä¿¡å¿ƒåº¦: {detection['confidence']:.2f}\n"
        info += f"é‚Šç•Œæ¡†: {bbox}"
        
        return annotated_frame, info
    
    def label_detection(self, video_path, detection_idx, category):
        """æ¨™è¨˜åµæ¸¬çµæœ"""
        if not video_path or video_path not in self.current_detections:
            return "âŒ è«‹å…ˆè™•ç†å½±ç‰‡"
        
        detections = self.current_detections[video_path]
        if not detections or detection_idx < 0 or detection_idx >= len(detections):
            return "âŒ åµæ¸¬ç´¢å¼•ç„¡æ•ˆ"
        
        if category not in self.categories:
            return f"âŒ ç„¡æ•ˆçš„é¡åˆ¥: {category}"
        
        try:
            detection = detections[detection_idx]
            frame = detection['frame']
            bbox = detection['bbox']
            
            # å»ºç«‹æª”æ¡ˆåç¨±
            video_name = Path(video_path).stem
            timestamp = detection['timestamp']
            filename = f"{video_name}_{timestamp:.1f}s_{detection['class_name']}.jpg"
            
            # å„²å­˜æ¨™è¨˜çš„å½±åƒ
            save_path = self.categories[category] / filename
            
            # åœ¨å½±åƒä¸Šç¹ªè£½é‚Šç•Œæ¡†
            annotated_frame = frame.copy()
            x1, y1, x2, y2 = bbox
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # æ·»åŠ æ¨™ç±¤
            label = f"{detection['class_name']} {detection['confidence']:.2f}"
            cv2.putText(annotated_frame, label, (x1, y1-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            cv2.imwrite(str(save_path), annotated_frame)
            
            # è¨˜éŒ„æ¨™è¨˜æ­·å²
            label_record = {
                'timestamp': datetime.now().isoformat(),
                'video_path': video_path,
                'detection_idx': detection_idx,
                'category': category,
                'detection_info': {
                    'timestamp': detection['timestamp'],
                    'class_name': detection['class_name'],
                    'confidence': detection['confidence'],
                    'bbox': bbox
                },
                'saved_path': str(save_path)
            }
            
            self.labeling_history.append(label_record)
            
            # å„²å­˜æ¨™è¨˜æ­·å²
            history_path = self.output_dir / "labeling_history.json"
            with open(history_path, 'w', encoding='utf-8') as f:
                json.dump(self.labeling_history, f, indent=2, ensure_ascii=False)
            
            return f"âœ… å·²æ¨™è¨˜ç‚º {category}ï¼Œå„²å­˜è‡³ {save_path}"
            
        except Exception as e:
            return f"âŒ æ¨™è¨˜æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
    
    def get_labeling_stats(self):
        """ç²å–æ¨™è¨˜çµ±è¨ˆ"""
        stats = defaultdict(int)
        for record in self.labeling_history:
            stats[record['category']] += 1
        
        total = sum(stats.values())
        stats_text = f"æ¨™è¨˜çµ±è¨ˆ (ç¸½è¨ˆ: {total}):\n"
        for category, count in stats.items():
            percentage = (count / total * 100) if total > 0 else 0
            stats_text += f"- {category}: {count} ({percentage:.1f}%)\n"
        
        return stats_text

def create_interface():
    """å»ºç«‹ Gradio ä»‹é¢"""
    labeling_tool = DataLabelingTool()
    
    with gr.Blocks(title="ç«ç½åµæ¸¬è³‡æ–™æ¨™è¨˜å·¥å…·", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸ”¥ ç«ç½åµæ¸¬è³‡æ–™æ¨™è¨˜å·¥å…·")
        gr.Markdown("é¸æ“‡ MP4 å½±ç‰‡é€²è¡Œç«ç½åµæ¸¬ï¼Œç„¶å¾Œæ¨™è¨˜åµæ¸¬çµæœçš„æ­£ç¢ºæ€§")
        
        with gr.Tab("å½±ç‰‡è™•ç†"):
            with gr.Row():
                with gr.Column(scale=1):
                    # å½±ç‰‡é¸æ“‡
                    video_dropdown = gr.Dropdown(
                        choices=labeling_tool.get_mp4_files(),
                        label="é¸æ“‡ MP4 å½±ç‰‡",
                        value=None
                    )
                    
                    refresh_btn = gr.Button("ğŸ”„ é‡æ–°æ•´ç†å½±ç‰‡åˆ—è¡¨", variant="secondary")
                    process_btn = gr.Button("ğŸ¬ è™•ç†å½±ç‰‡", variant="primary")
                    
                    # å½±ç‰‡è³‡è¨Š
                    video_info = gr.Textbox(
                        label="å½±ç‰‡è³‡è¨Š",
                        lines=6,
                        interactive=False
                    )
                
                with gr.Column(scale=2):
                    # åµæ¸¬çµæœåˆ—è¡¨
                    detection_list = gr.Dropdown(
                        choices=[],
                        label="åµæ¸¬çµæœ",
                        value=None
                    )
                    
                    show_detection_btn = gr.Button("ğŸ‘ï¸ é¡¯ç¤ºåµæ¸¬çµæœ", variant="secondary")
                    
                    # é¡¯ç¤ºåµæ¸¬å½±åƒ
                    detection_image = gr.Image(
                        label="åµæ¸¬çµæœ",
                        type="numpy"
                    )
                    
                    detection_info = gr.Textbox(
                        label="åµæ¸¬è©³æƒ…",
                        lines=4,
                        interactive=False
                    )
        
        with gr.Tab("æ¨™è¨˜"):
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### ğŸ·ï¸ é¸æ“‡æ­£ç¢ºçš„é¡åˆ¥")
                    
                    with gr.Row():
                        true_fire_btn = gr.Button("âœ… çœŸå¯¦ç«ç½", variant="primary")
                        true_smoke_btn = gr.Button("âœ… çœŸå¯¦ç…™éœ§", variant="primary")
                    
                    with gr.Row():
                        false_cloud_btn = gr.Button("âŒ é›²æœµèª¤åˆ¤", variant="secondary")
                        false_light_btn = gr.Button("âŒ ç‡ˆå…‰èª¤åˆ¤", variant="secondary")
                    
                    with gr.Row():
                        false_static_btn = gr.Button("âŒ éœæ…‹ç‰©é«”", variant="secondary")
                        uncertain_btn = gr.Button("â“ ä¸ç¢ºå®š", variant="stop")
                    
                    label_result = gr.Textbox(
                        label="æ¨™è¨˜çµæœ",
                        lines=3,
                        interactive=False
                    )
                
                with gr.Column():
                    gr.Markdown("### ğŸ“Š æ¨™è¨˜çµ±è¨ˆ")
                    stats_display = gr.Textbox(
                        label="çµ±è¨ˆè³‡è¨Š",
                        lines=8,
                        interactive=False
                    )
                    
                    refresh_stats_btn = gr.Button("ğŸ”„ æ›´æ–°çµ±è¨ˆ", variant="secondary")
        
        # éš±è—ç‹€æ…‹
        current_video = gr.State(value="")
        current_detection_idx = gr.State(value=-1)
        
        # äº‹ä»¶è™•ç†
        def refresh_video_list():
            return gr.Dropdown(choices=labeling_tool.get_mp4_files())
        
        def process_video_wrapper(video_path):
            info, full_path, detections = labeling_tool.process_video(video_path)
            return info, video_path, gr.Dropdown(choices=detections), detections
        
        def show_detection_wrapper(video_path, detection_selection):
            if not detection_selection:
                return None, "è«‹é¸æ“‡ä¸€å€‹åµæ¸¬çµæœ"
            
            # è§£æé¸æ“‡çš„ç´¢å¼•
            try:
                detection_idx = int(detection_selection.split(']')[0].split('[')[1]) - 1
                image, info = labeling_tool.show_detection(video_path, detection_idx)
                return image, info, detection_idx
            except:
                return None, "è§£æåµæ¸¬ç´¢å¼•å¤±æ•—", -1
        
        def label_wrapper(video_path, detection_idx, category):
            result = labeling_tool.label_detection(video_path, detection_idx, category)
            stats = labeling_tool.get_labeling_stats()
            return result, stats
        
        # ç¶å®šäº‹ä»¶
        refresh_btn.click(refresh_video_list, outputs=[video_dropdown])
        
        process_btn.click(
            process_video_wrapper,
            inputs=[video_dropdown],
            outputs=[video_info, current_video, detection_list, gr.State()]
        )
        
        show_detection_btn.click(
            show_detection_wrapper,
            inputs=[current_video, detection_list],
            outputs=[detection_image, detection_info, current_detection_idx]
        )
        
        # æ¨™è¨˜æŒ‰éˆ•
        for category, btn in [
            ("true_fire", true_fire_btn),
            ("true_smoke", true_smoke_btn),
            ("false_cloud", false_cloud_btn),
            ("false_light", false_light_btn),
            ("false_static", false_static_btn),
            ("uncertain", uncertain_btn)
        ]:
            btn.click(
                lambda video, idx, cat=category: label_wrapper(video, idx, cat),
                inputs=[current_video, current_detection_idx],
                outputs=[label_result, stats_display]
            )
        
        refresh_stats_btn.click(
            labeling_tool.get_labeling_stats,
            outputs=[stats_display]
        )
    
    return interface

def main():
    """å•Ÿå‹•æ¨™è¨˜ä»‹é¢"""
    print("ğŸš€ å•Ÿå‹•è³‡æ–™æ¨™è¨˜ä»‹é¢...")
    
    # æª¢æŸ¥å¿…è¦å¥—ä»¶
    if not ULTRALYTICS_AVAILABLE:
        print("âš ï¸  è«‹å®‰è£ ultralytics: pip install ultralytics")
    
    try:
        interface = create_interface()
        interface.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=False,
            show_error=True
        )
    except Exception as e:
        print(f"âŒ å•Ÿå‹•ä»‹é¢å¤±æ•—: {e}")

if __name__ == "__main__":
    main()
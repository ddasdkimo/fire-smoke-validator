#!/usr/bin/env python3
"""
TIMM æ¨¡å‹åˆ° TensorRT è½‰æ› Gradio ä»‹é¢
æ”¯æ´ .pth æ¨¡å‹è½‰æ›ç‚º .engine/.plan æ ¼å¼
"""

import gradio as gr
import os
import json
import tempfile
import shutil
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple, List, Dict, Any
import zipfile

from utils.model_converter import TimmToTensorRTConverter, get_supported_timm_models

class TensorRTConverterApp:
    """TensorRT è½‰æ›æ‡‰ç”¨ç¨‹å¼"""
    
    def __init__(self):
        self.converter = TimmToTensorRTConverter(verbose=True)
        self.models_dir = Path("/app/models")
        self.converted_dir = Path("/app/converted_models")
        self.temp_dir = Path("/app/temp")
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        for dir_path in [self.models_dir, self.converted_dir, self.temp_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    def convert_model(self,
                     model_file: gr.File,
                     model_name: str,
                     num_classes: int,
                     max_batch_size: int,
                     enable_fp16: bool,
                     input_height: int,
                     input_width: int) -> Tuple[str, str, Optional[str]]:
        """æ¨¡å‹è½‰æ›ä¸»å‡½æ•¸"""
        
        if model_file is None:
            return "âŒ è«‹ä¸Šå‚³ .pth æ¨¡å‹æª”æ¡ˆ", "", None
        
        try:
            # ç”Ÿæˆæ™‚é–“æˆ³
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # è¤‡è£½ä¸Šå‚³çš„æª”æ¡ˆåˆ°è‡¨æ™‚ç›®éŒ„
            temp_model_path = self.temp_dir / f"temp_model_{timestamp}.pth"
            shutil.copy2(model_file.name, temp_model_path)
            
            # è¨­å®šè¼¸å‡ºè·¯å¾‘
            model_basename = Path(model_file.name).stem
            output_name = f"{model_basename}_{model_name}_{timestamp}"
            output_path = self.converted_dir / f"{output_name}.engine"
            
            # åŸ·è¡Œè½‰æ›
            status_msg = f"ğŸ”„ é–‹å§‹è½‰æ›æ¨¡å‹...\n"
            status_msg += f"ğŸ“ æ¨¡å‹åç¨±: {model_name}\n"
            status_msg += f"ğŸ¯ é¡åˆ¥æ•¸é‡: {num_classes}\n"
            status_msg += f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {max_batch_size}\n"
            status_msg += f"âš¡ FP16 åŠ é€Ÿ: {'å•Ÿç”¨' if enable_fp16 else 'åœç”¨'}\n"
            status_msg += f"ğŸ–¼ï¸ è¼¸å…¥å°ºå¯¸: {input_height}x{input_width}\n\n"
            
            input_shape = (max_batch_size, 3, input_height, input_width)
            
            results = self.converter.convert_pth_to_tensorrt(
                pth_path=str(temp_model_path),
                output_path=str(output_path),
                model_name=model_name,
                num_classes=num_classes,
                input_shape=input_shape,
                max_batch_size=max_batch_size,
                fp16_mode=enable_fp16
            )
            
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            if temp_model_path.exists():
                temp_model_path.unlink()
            
            if results['success']:
                # ç²å–å¼•æ“è³‡è¨Š
                engine_info = self.converter.get_model_info(results['engine_path'])
                
                success_msg = f"ğŸ‰ æ¨¡å‹è½‰æ›æˆåŠŸï¼\n\n"
                success_msg += f"ğŸ“ è¼¸å‡ºæª”æ¡ˆ: {Path(results['engine_path']).name}\n"
                success_msg += f"ğŸ’¾ æª”æ¡ˆå¤§å°: {self._get_file_size(results['engine_path'])}\n\n"
                
                # æ¨¡å‹è³‡è¨Š
                if 'model_info' in results:
                    info = results['model_info']
                    success_msg += f"ğŸ¤– æ¨¡å‹è³‡è¨Š:\n"
                    success_msg += f"- æ¨¡å‹æ¶æ§‹: {info.get('model_name', 'unknown')}\n"
                    success_msg += f"- ç¸½åƒæ•¸é‡: {info.get('total_parameters', 0):,}\n"
                    success_msg += f"- å¯è¨“ç·´åƒæ•¸: {info.get('trainable_parameters', 0):,}\n"
                    success_msg += f"- é¡åˆ¥æ•¸é‡: {info.get('num_classes', num_classes)}\n\n"
                
                # TensorRT å¼•æ“è³‡è¨Š
                if 'error' not in engine_info:
                    success_msg += f"âš¡ TensorRT å¼•æ“è³‡è¨Š:\n"
                    success_msg += f"- æœ€å¤§æ‰¹æ¬¡å¤§å°: {engine_info.get('max_batch_size', 'N/A')}\n"
                    success_msg += f"- è¨­å‚™è¨˜æ†¶é«”: {engine_info.get('device_memory_size', 0) / (1024**2):.1f} MB\n"
                    success_msg += f"- å·¥ä½œç©ºé–“å¤§å°: {engine_info.get('workspace_size', 0) / (1024**2):.1f} MB\n"
                    success_msg += f"- ç¹«çµæ•¸é‡: {engine_info.get('num_bindings', 0)}\n"
                
                return success_msg, self._create_download_info(results['engine_path']), results['engine_path']
            else:
                error_msg = f"âŒ è½‰æ›å¤±æ•—:\n{results.get('error', 'æœªçŸ¥éŒ¯èª¤')}"
                return error_msg, "", None
                
        except Exception as e:
            error_msg = f"âŒ è½‰æ›éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            return error_msg, "", None
    
    def _get_file_size(self, file_path: str) -> str:
        """å–å¾—æª”æ¡ˆå¤§å°çš„å¯è®€æ ¼å¼"""
        try:
            size_bytes = os.path.getsize(file_path)
            if size_bytes < 1024:
                return f"{size_bytes} B"
            elif size_bytes < 1024**2:
                return f"{size_bytes/1024:.1f} KB"
            elif size_bytes < 1024**3:
                return f"{size_bytes/(1024**2):.1f} MB"
            else:
                return f"{size_bytes/(1024**3):.1f} GB"
        except:
            return "Unknown"
    
    def _create_download_info(self, engine_path: str) -> str:
        """å»ºç«‹ä¸‹è¼‰è³‡è¨Š"""
        file_name = Path(engine_path).name
        file_size = self._get_file_size(engine_path)
        
        info = f"""ğŸ“¦ ä¸‹è¼‰è³‡è¨Š:
        
æª”æ¡ˆåç¨±: {file_name}
æª”æ¡ˆå¤§å°: {file_size}
æ ¼å¼: TensorRT Engine (.engine)

âœ… æª”æ¡ˆå·²æº–å‚™å°±ç·’ï¼Œå¯ä»¥ä¸‹è¼‰ä½¿ç”¨ï¼

ğŸ’¡ ä½¿ç”¨èªªæ˜:
1. ä¸‹è¼‰ .engine æª”æ¡ˆ
2. åœ¨æ‚¨çš„ TensorRT æ‡‰ç”¨ç¨‹å¼ä¸­è¼‰å…¥æ­¤å¼•æ“
3. ç¢ºä¿æ¨è«–æ™‚ä½¿ç”¨ç›¸åŒçš„è¼¸å…¥å°ºå¯¸å’Œæ‰¹æ¬¡å¤§å°
"""
        return info
    
    def get_converted_models(self) -> List[Tuple[str, str]]:
        """å–å¾—å·²è½‰æ›çš„æ¨¡å‹åˆ—è¡¨"""
        models = []
        try:
            for file_path in self.converted_dir.glob("*.engine"):
                file_size = self._get_file_size(str(file_path))
                models.append((file_path.name, f"{file_path.name} ({file_size})"))
        except Exception as e:
            print(f"ç²å–æ¨¡å‹åˆ—è¡¨å¤±æ•—: {e}")
        
        return models
    
    def delete_model(self, model_name: str) -> str:
        """åˆªé™¤å·²è½‰æ›çš„æ¨¡å‹"""
        if not model_name:
            return "âŒ è«‹é¸æ“‡è¦åˆªé™¤çš„æ¨¡å‹"
        
        try:
            model_path = self.converted_dir / model_name
            if model_path.exists():
                model_path.unlink()
                return f"âœ… å·²åˆªé™¤æ¨¡å‹: {model_name}"
            else:
                return f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_name}"
        except Exception as e:
            return f"âŒ åˆªé™¤å¤±æ•—: {str(e)}"
    
    def create_interface(self) -> gr.Interface:
        """å»ºç«‹ Gradio ä»‹é¢"""
        
        # å–å¾—æ”¯æ´çš„æ¨¡å‹åˆ—è¡¨
        supported_models = get_supported_timm_models()
        
        with gr.Blocks(
            title="TIMM to TensorRT Converter",
            theme=gr.themes.Soft(),
            css="""
            .main-header { text-align: center; margin-bottom: 2rem; }
            .conversion-section { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                                 padding: 2rem; border-radius: 1rem; margin: 1rem 0; }
            .status-box { background: #f8f9fa; padding: 1rem; border-radius: 0.5rem; 
                         border-left: 4px solid #28a745; margin: 1rem 0; }
            """
        ) as interface:
            
            # æ¨™é¡Œ
            gr.HTML("""
            <div class="main-header">
                <h1>ğŸš€ TIMM to TensorRT Converter</h1>
                <p>å°‡ TIMM çš„ .pth æ¨¡å‹è½‰æ›ç‚º TensorRT .engine/.plan æ ¼å¼ï¼Œæä¾›é«˜æ•ˆèƒ½æ¨è«–åŠ é€Ÿ</p>
            </div>
            """)
            
            with gr.Tab("ğŸ”„ æ¨¡å‹è½‰æ›", elem_id="conversion-tab"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.HTML("<h3>ğŸ“¥ è¼¸å…¥è¨­å®š</h3>")
                        
                        model_file = gr.File(
                            label="ä¸Šå‚³ .pth æ¨¡å‹æª”æ¡ˆ",
                            file_types=[".pth", ".pt"],
                            type="filepath"
                        )
                        
                        model_name = gr.Dropdown(
                            label="é¸æ“‡æ¨¡å‹æ¶æ§‹",
                            choices=supported_models,
                            value="efficientnet_b0",
                            allow_custom_value=True,
                            info="é¸æ“‡å°æ‡‰çš„ TIMM æ¨¡å‹æ¶æ§‹"
                        )
                        
                        with gr.Row():
                            num_classes = gr.Number(
                                label="é¡åˆ¥æ•¸é‡",
                                value=2,
                                minimum=1,
                                maximum=10000,
                                step=1
                            )
                            
                            max_batch_size = gr.Number(
                                label="æœ€å¤§æ‰¹æ¬¡å¤§å°",
                                value=1,
                                minimum=1,
                                maximum=64,
                                step=1
                            )
                        
                        gr.HTML("<h4>ğŸ–¼ï¸ è¼¸å…¥å°ºå¯¸è¨­å®š</h4>")
                        with gr.Row():
                            input_height = gr.Number(
                                label="è¼¸å…¥é«˜åº¦",
                                value=224,
                                minimum=32,
                                maximum=1024,
                                step=32
                            )
                            
                            input_width = gr.Number(
                                label="è¼¸å…¥å¯¬åº¦",
                                value=224,
                                minimum=32,
                                maximum=1024,
                                step=32
                            )
                        
                        enable_fp16 = gr.Checkbox(
                            label="å•Ÿç”¨ FP16 ç²¾åº¦ï¼ˆåŠ é€Ÿæ¨è«–ï¼‰",
                            value=True,
                            info="åœ¨æ”¯æ´çš„ GPU ä¸Šå¯æä¾› 2x åŠ é€Ÿ"
                        )
                        
                        convert_btn = gr.Button(
                            "ğŸš€ é–‹å§‹è½‰æ›",
                            variant="primary",
                            size="lg"
                        )
                    
                    with gr.Column(scale=1):
                        gr.HTML("<h3>ğŸ“Š è½‰æ›çµæœ</h3>")
                        
                        status_output = gr.Textbox(
                            label="è½‰æ›ç‹€æ…‹",
                            lines=15,
                            max_lines=20,
                            interactive=False
                        )
                        
                        download_info = gr.Textbox(
                            label="ä¸‹è¼‰è³‡è¨Š",
                            lines=8,
                            interactive=False
                        )
                        
                        download_file = gr.File(
                            label="ä¸‹è¼‰è½‰æ›å¾Œçš„ .engine æª”æ¡ˆ",
                            interactive=False
                        )
            
            with gr.Tab("ğŸ“ ç®¡ç†æ¨¡å‹", elem_id="management-tab"):
                with gr.Row():
                    with gr.Column(scale=2):
                        gr.HTML("<h3>ğŸ“‚ å·²è½‰æ›çš„æ¨¡å‹</h3>")
                        
                        model_list = gr.Dropdown(
                            label="é¸æ“‡æ¨¡å‹",
                            choices=[],
                            interactive=True
                        )
                        
                        refresh_btn = gr.Button("ğŸ”„ é‡æ–°æ•´ç†åˆ—è¡¨")
                        delete_btn = gr.Button("ğŸ—‘ï¸ åˆªé™¤é¸ä¸­æ¨¡å‹", variant="stop")
                        
                        delete_result = gr.Textbox(
                            label="æ“ä½œçµæœ",
                            lines=3,
                            interactive=False
                        )
                    
                    with gr.Column(scale=1):
                        gr.HTML("""
                        <div class="status-box">
                            <h4>ğŸ’¡ ä½¿ç”¨èªªæ˜</h4>
                            <ul>
                                <li>æ”¯æ´æ‰€æœ‰ TIMM é è¨“ç·´æ¨¡å‹</li>
                                <li>è‡ªå‹•é€²è¡Œ FP16 æœ€ä½³åŒ–</li>
                                <li>æ”¯æ´å‹•æ…‹æ‰¹æ¬¡å¤§å°</li>
                                <li>è¼¸å‡º TensorRT .engine æ ¼å¼</li>
                                <li>å¯ç›´æ¥ç”¨æ–¼ Triton æ¨è«–ä¼ºæœå™¨</li>
                            </ul>
                        </div>
                        """)
            
            with gr.Tab("â„¹ï¸ ç³»çµ±è³‡è¨Š", elem_id="info-tab"):
                gr.HTML("""
                <div class="status-box">
                    <h3>ğŸ”§ æ”¯æ´çš„åŠŸèƒ½</h3>
                    <ul>
                        <li><strong>æ¨¡å‹æ ¼å¼</strong>: TIMM .pth/.pt æ¨¡å‹</li>
                        <li><strong>è¼¸å‡ºæ ¼å¼</strong>: TensorRT .engine/.plan</li>
                        <li><strong>æœ€ä½³åŒ–</strong>: FP16 ç²¾åº¦ã€å‹•æ…‹å½¢ç‹€ã€è¨˜æ†¶é«”æœ€ä½³åŒ–</li>
                        <li><strong>æ”¯æ´æ¶æ§‹</strong>: ResNet, EfficientNet, ConvNeXt, Swin, ViT, ç­‰</li>
                    </ul>
                    
                    <h3>âš¡ æ•ˆèƒ½å„ªå‹¢</h3>
                    <ul>
                        <li>æ¯”åŸå§‹ PyTorch æ¨¡å‹å¿« 2-10 å€</li>
                        <li>æ”¯æ´ FP16 æ··åˆç²¾åº¦åŠ é€Ÿ</li>
                        <li>è¨˜æ†¶é«”ä½¿ç”¨é‡æœ€ä½³åŒ–</li>
                        <li>æ”¯æ´æ‰¹æ¬¡è™•ç†æœ€ä½³åŒ–</li>
                    </ul>
                </div>
                """)
            
            # äº‹ä»¶ç¶å®š
            convert_btn.click(
                fn=self.convert_model,
                inputs=[
                    model_file, model_name, num_classes, max_batch_size,
                    enable_fp16, input_height, input_width
                ],
                outputs=[status_output, download_info, download_file]
            )
            
            refresh_btn.click(
                fn=lambda: gr.Dropdown(choices=[name for name, _ in self.get_converted_models()]),
                outputs=model_list
            )
            
            delete_btn.click(
                fn=self.delete_model,
                inputs=model_list,
                outputs=delete_result
            )
            
            # åˆå§‹åŒ–æ¨¡å‹åˆ—è¡¨
            interface.load(
                fn=lambda: gr.Dropdown(choices=[name for name, _ in self.get_converted_models()]),
                outputs=model_list
            )
        
        return interface


def main():
    """ä¸»ç¨‹åº"""
    print("ğŸš€ å•Ÿå‹• TIMM to TensorRT è½‰æ›å™¨...")
    
    app = TensorRTConverterApp()
    interface = app.create_interface()
    
    # å•Ÿå‹• Gradio ä»‹é¢
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=False,
        show_error=True,
        show_tips=True
    )


if __name__ == "__main__":
    main()
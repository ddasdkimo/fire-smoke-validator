#!/usr/bin/env python3
"""
TIMM æ¨¡å‹åˆ° TensorRT è½‰æ›å·¥å…·
æ”¯æ´å°‡ .pth æ¨¡å‹è½‰æ›ç‚º .engine å’Œ .plan æ ¼å¼
"""

import os
import torch
import timm
import tensorrt as trt
import numpy as np
from pathlib import Path
import json
import logging
from typing import Optional, Tuple, List, Dict, Any
import tempfile
import onnx

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TimmToTensorRTConverter:
    """TIMM æ¨¡å‹åˆ° TensorRT è½‰æ›å™¨"""
    
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.trt_logger = trt.Logger(trt.Logger.INFO if verbose else trt.Logger.WARNING)
        
    def load_timm_model(self, 
                       model_path: str, 
                       model_name: Optional[str] = None,
                       num_classes: int = 2) -> torch.nn.Module:
        """è¼‰å…¥ TIMM æ¨¡å‹"""
        try:
            if model_path.endswith('.pth') or model_path.endswith('.pt'):
                # è¼‰å…¥é è¨“ç·´çš„æ¨¡å‹
                if model_name:
                    # å‰µå»ºæ¨¡å‹æ¶æ§‹ç„¶å¾Œè¼‰å…¥æ¬Šé‡
                    model = timm.create_model(model_name, num_classes=num_classes)
                    state_dict = torch.load(model_path, map_location='cpu')
                    
                    # è™•ç†å¯èƒ½çš„ç‹€æ…‹å­—å…¸æ ¼å¼
                    if 'state_dict' in state_dict:
                        state_dict = state_dict['state_dict']
                    elif 'model' in state_dict:
                        state_dict = state_dict['model']
                    
                    model.load_state_dict(state_dict, strict=False)
                else:
                    # ç›´æ¥è¼‰å…¥å®Œæ•´æ¨¡å‹
                    model = torch.load(model_path, map_location='cpu')
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹æ ¼å¼: {model_path}")
            
            model.eval()
            logger.info(f"âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹: {model_path}")
            return model
            
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
            raise
    
    def convert_to_onnx(self, 
                       model: torch.nn.Module,
                       input_shape: Tuple[int, int, int, int] = (1, 3, 224, 224),
                       output_path: str = "temp_model.onnx",
                       opset_version: int = 11) -> str:
        """å°‡ PyTorch æ¨¡å‹è½‰æ›ç‚º ONNX"""
        try:
            # å‰µå»ºç¤ºä¾‹è¼¸å…¥
            dummy_input = torch.randn(*input_shape)
            
            # å°å‡ºç‚º ONNX
            torch.onnx.export(
                model,
                dummy_input,
                output_path,
                export_params=True,
                opset_version=opset_version,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # é©—è­‰ ONNX æ¨¡å‹
            onnx_model = onnx.load(output_path)
            onnx.checker.check_model(onnx_model)
            
            logger.info(f"âœ… æˆåŠŸè½‰æ›ç‚º ONNX: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"âŒ ONNX è½‰æ›å¤±æ•—: {e}")
            raise
    
    def build_tensorrt_engine(self,
                            onnx_path: str,
                            engine_path: str,
                            max_batch_size: int = 1,
                            fp16_mode: bool = True,
                            max_workspace_size: int = 1 << 30) -> str:  # 1GB
        """å°‡ ONNX æ¨¡å‹è½‰æ›ç‚º TensorRT å¼•æ“"""
        try:
            # å‰µå»º TensorRT builder
            builder = trt.Builder(self.trt_logger)
            
            # å‰µå»ºç¶²è·¯å®šç¾©
            network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
            
            # å‰µå»º ONNX è§£æå™¨
            parser = trt.OnnxParser(network, self.trt_logger)
            
            # è§£æ ONNX æ¨¡å‹
            with open(onnx_path, 'rb') as model_file:
                if not parser.parse(model_file.read()):
                    logger.error("âŒ ONNX è§£æå¤±æ•—")
                    for error in range(parser.num_errors):
                        logger.error(parser.get_error(error))
                    raise RuntimeError("ONNX è§£æå¤±æ•—")
            
            # å‰µå»ºæ§‹å»ºé…ç½®
            config = builder.create_builder_config()
            config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, max_workspace_size)
            
            # å•Ÿç”¨ FP16 ç²¾åº¦ï¼ˆå¦‚æœæ”¯æ´ï¼‰
            if fp16_mode and builder.platform_has_fast_fp16:
                config.set_flag(trt.BuilderFlag.FP16)
                logger.info("ğŸš€ å•Ÿç”¨ FP16 ç²¾åº¦åŠ é€Ÿ")
            
            # è¨­å®šå‹•æ…‹è¼¸å…¥å½¢ç‹€
            profile = builder.create_optimization_profile()
            input_tensor = network.get_input(0)
            
            # è¨­å®šæœ€å°ã€æœ€ä½³ã€æœ€å¤§è¼¸å…¥å½¢ç‹€
            min_shape = (1, 3, 224, 224)
            opt_shape = (max_batch_size, 3, 224, 224)
            max_shape = (max_batch_size * 2, 3, 224, 224)
            
            profile.set_shape(input_tensor.name, min_shape, opt_shape, max_shape)
            config.add_optimization_profile(profile)
            
            # æ§‹å»ºå¼•æ“
            logger.info("ğŸ”„ é–‹å§‹æ§‹å»º TensorRT å¼•æ“...")
            serialized_engine = builder.build_serialized_network(network, config)
            
            if serialized_engine is None:
                raise RuntimeError("TensorRT å¼•æ“æ§‹å»ºå¤±æ•—")
            
            # å„²å­˜å¼•æ“
            with open(engine_path, 'wb') as f:
                f.write(serialized_engine)
            
            logger.info(f"âœ… æˆåŠŸå»ºç«‹ TensorRT å¼•æ“: {engine_path}")
            return engine_path
            
        except Exception as e:
            logger.error(f"âŒ TensorRT å¼•æ“æ§‹å»ºå¤±æ•—: {e}")
            raise
    
    def convert_pth_to_tensorrt(self,
                              pth_path: str,
                              output_path: str,
                              model_name: Optional[str] = None,
                              num_classes: int = 2,
                              input_shape: Tuple[int, int, int, int] = (1, 3, 224, 224),
                              max_batch_size: int = 1,
                              fp16_mode: bool = True) -> Dict[str, Any]:
        """å®Œæ•´çš„ .pth åˆ° TensorRT è½‰æ›æµç¨‹"""
        
        results = {
            'success': False,
            'onnx_path': None,
            'engine_path': None,
            'plan_path': None,
            'error': None,
            'model_info': {}
        }
        
        temp_onnx_path = None
        
        try:
            # 1. è¼‰å…¥ TIMM æ¨¡å‹
            logger.info("ğŸ“¥ è¼‰å…¥ TIMM æ¨¡å‹...")
            model = self.load_timm_model(pth_path, model_name, num_classes)
            
            # ç²å–æ¨¡å‹è³‡è¨Š
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            results['model_info'] = {
                'total_parameters': total_params,
                'trainable_parameters': trainable_params,
                'model_name': model_name or "unknown",
                'num_classes': num_classes
            }
            
            # 2. è½‰æ›ç‚º ONNX
            logger.info("ğŸ”„ è½‰æ›ç‚º ONNX æ ¼å¼...")
            with tempfile.NamedTemporaryFile(suffix='.onnx', delete=False) as tmp_file:
                temp_onnx_path = tmp_file.name
            
            self.convert_to_onnx(
                model, 
                input_shape=input_shape,
                output_path=temp_onnx_path
            )
            results['onnx_path'] = temp_onnx_path
            
            # 3. è½‰æ›ç‚º TensorRT å¼•æ“
            logger.info("âš¡ è½‰æ›ç‚º TensorRT å¼•æ“...")
            
            # ç¢ºå®šè¼¸å‡ºè·¯å¾‘
            if output_path.endswith('.engine') or output_path.endswith('.plan'):
                engine_path = output_path
            else:
                engine_path = f"{output_path}.engine"
            
            self.build_tensorrt_engine(
                temp_onnx_path,
                engine_path,
                max_batch_size=max_batch_size,
                fp16_mode=fp16_mode
            )
            
            results['engine_path'] = engine_path
            results['plan_path'] = engine_path  # .engine å’Œ .plan æœ¬è³ªä¸Šç›¸åŒ
            results['success'] = True
            
            logger.info("ğŸ‰ æ¨¡å‹è½‰æ›å®Œæˆï¼")
            
        except Exception as e:
            error_msg = f"æ¨¡å‹è½‰æ›å¤±æ•—: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            results['error'] = error_msg
            
        finally:
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            if temp_onnx_path and os.path.exists(temp_onnx_path):
                try:
                    os.unlink(temp_onnx_path)
                except:
                    pass
        
        return results
    
    def get_model_info(self, engine_path: str) -> Dict[str, Any]:
        """ç²å– TensorRT å¼•æ“è³‡è¨Š"""
        try:
            with open(engine_path, 'rb') as f:
                engine_data = f.read()
            
            runtime = trt.Runtime(self.trt_logger)
            engine = runtime.deserialize_cuda_engine(engine_data)
            
            info = {
                'num_bindings': engine.num_bindings,
                'max_batch_size': engine.max_batch_size,
                'device_memory_size': engine.device_memory_size,
                'workspace_size': engine.workspace_size,
                'bindings': []
            }
            
            for i in range(engine.num_bindings):
                binding_info = {
                    'name': engine.get_binding_name(i),
                    'shape': engine.get_binding_shape(i),
                    'dtype': str(engine.get_binding_dtype(i)),
                    'is_input': engine.binding_is_input(i)
                }
                info['bindings'].append(binding_info)
            
            return info
            
        except Exception as e:
            logger.error(f"âŒ ç²å–å¼•æ“è³‡è¨Šå¤±æ•—: {e}")
            return {'error': str(e)}


def get_supported_timm_models() -> List[str]:
    """ç²å–æ”¯æ´çš„ TIMM æ¨¡å‹åˆ—è¡¨"""
    # å¸¸ç”¨çš„ TIMM æ¨¡å‹
    popular_models = [
        'resnet50', 'resnet101', 'resnet152',
        'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3',
        'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7',
        'efficientnetv2_s', 'efficientnetv2_m', 'efficientnetv2_l',
        'mobilenetv3_small_100', 'mobilenetv3_large_100',
        'convnext_tiny', 'convnext_small', 'convnext_base', 'convnext_large',
        'swin_tiny_patch4_window7_224', 'swin_small_patch4_window7_224',
        'swin_base_patch4_window7_224', 'swin_large_patch4_window7_224',
        'vit_base_patch16_224', 'vit_large_patch16_224',
        'deit_tiny_patch16_224', 'deit_small_patch16_224', 'deit_base_patch16_224'
    ]
    
    return popular_models


if __name__ == "__main__":
    # æ¸¬è©¦è½‰æ›å™¨
    converter = TimmToTensorRTConverter(verbose=True)
    
    # åˆ—å‡ºæ”¯æ´çš„æ¨¡å‹
    models = get_supported_timm_models()
    print("ğŸ¤– æ”¯æ´çš„ TIMM æ¨¡å‹:")
    for i, model in enumerate(models[:10]):  # åªé¡¯ç¤ºå‰10å€‹
        print(f"  {i+1}. {model}")
    print(f"  ... ç¸½å…± {len(models)} å€‹æ¨¡å‹")
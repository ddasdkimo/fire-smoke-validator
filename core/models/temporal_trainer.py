#!/usr/bin/env python3
"""
æ™‚åºæ¨¡å‹è¨“ç·´å™¨
è™•ç†æ™‚åºè³‡æ–™çš„è¼‰å…¥ã€è¨“ç·´å’Œè©•ä¼°
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import cv2
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from datetime import datetime

try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False

from .temporal_classifier import TemporalFireSmokeClassifier, DEFAULT_MODEL_CONFIGS
from .data_utils import prepare_temporal_frames, create_temporal_batch


class TemporalFireSmokeDataset(Dataset):
    """
    æ™‚åºç«ç…™è³‡æ–™é›†
    å¾æ¨™è¨»çš„äº‹ä»¶è³‡æ–™è¼‰å…¥æ™‚åºå¹€åºåˆ—
    """
    
    def __init__(self, 
                 dataset_path: str,
                 temporal_frames: int = 5,
                 image_size: Tuple[int, int] = (224, 224),
                 training: bool = True):
        """
        Args:
            dataset_path: è³‡æ–™é›†è·¯å¾‘
            temporal_frames: æ™‚åºå¹€æ•¸
            image_size: å½±åƒå°ºå¯¸
            training: æ˜¯å¦ç‚ºè¨“ç·´æ¨¡å¼
        """
        self.dataset_path = Path(dataset_path)
        self.temporal_frames = temporal_frames
        self.image_size = image_size
        self.training = training
        
        # é¡åˆ¥æ˜ å°„ - å¿…é ˆåœ¨ _load_dataset ä¹‹å‰åˆå§‹åŒ–
        self.class_to_idx = {'false_positive': 0, 'true_positive': 1}
        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}
        
        # è¼‰å…¥è³‡æ–™
        self.samples = self._load_dataset()
        
        print(f"âœ… è¼‰å…¥ {len(self.samples)} å€‹æ™‚åºæ¨£æœ¬")
        self._print_dataset_stats()
    
    def _load_dataset(self) -> List[Dict[str, Any]]:
        """è¼‰å…¥è³‡æ–™é›†æ¨£æœ¬"""
        samples = []
        
        for class_name in ['true_positive', 'false_positive']:
            class_dir = self.dataset_path / class_name
            if not class_dir.exists():
                continue
                
            for event_dir in class_dir.iterdir():
                if not event_dir.is_dir():
                    continue
                
                # æŸ¥æ‰¾äº‹ä»¶ä¸­çš„æ‰€æœ‰å½±åƒ
                image_files = []
                for ext in ['*.jpg', '*.jpeg', '*.png']:
                    image_files.extend(list(event_dir.glob(ext)))
                
                if len(image_files) == 0:
                    continue
                
                # æŒ‰æª”åæ’åºç¢ºä¿æ™‚åºæ­£ç¢º
                image_files.sort(key=lambda x: x.name)
                
                # è¼‰å…¥ metadata (å¦‚æœå­˜åœ¨)
                metadata_path = event_dir / 'metadata.json'
                metadata = {}
                if metadata_path.exists():
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                    except:
                        pass
                
                samples.append({
                    'event_dir': str(event_dir),
                    'image_files': [str(f) for f in image_files],
                    'class_name': class_name,
                    'label': self.class_to_idx[class_name],
                    'metadata': metadata
                })
        
        return samples
    
    def _print_dataset_stats(self):
        """å°å‡ºè³‡æ–™é›†çµ±è¨ˆ"""
        stats = {}
        for sample in self.samples:
            class_name = sample['class_name']
            if class_name not in stats:
                stats[class_name] = {'count': 0, 'total_frames': 0}
            stats[class_name]['count'] += 1
            stats[class_name]['total_frames'] += len(sample['image_files'])
        
        print("ğŸ“Š è³‡æ–™é›†çµ±è¨ˆ:")
        for class_name, stat in stats.items():
            avg_frames = stat['total_frames'] / stat['count']
            print(f"  {class_name}: {stat['count']} äº‹ä»¶, å¹³å‡ {avg_frames:.1f} å¹€")
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        """
        ç²å–å–®å€‹æ¨£æœ¬
        
        Returns:
            Tuple[torch.Tensor, int]: (frames [T, C, H, W], label)
        """
        sample = self.samples[idx]
        
        # è¼‰å…¥æ‰€æœ‰å¹€
        frames = []
        for image_path in sample['image_files']:
            frame = cv2.imread(image_path)
            if frame is not None:
                frames.append(frame)
        
        if not frames:
            # å¦‚æœç„¡æ³•è¼‰å…¥ä»»ä½•å¹€ï¼Œè¿”å›é»‘è‰²å¹€
            frames = [np.zeros((224, 224, 3), dtype=np.uint8)]
        
        # æº–å‚™æ™‚åºå¹€åºåˆ—
        temporal_frames = prepare_temporal_frames(
            frames, 
            target_frames=self.temporal_frames,
            image_size=self.image_size,
            training=self.training
        )
        
        return temporal_frames, sample['label']


class TemporalTrainer:
    """
    æ™‚åºæ¨¡å‹è¨“ç·´å™¨
    """
    
    def __init__(self, 
                 model_config: Dict[str, Any],
                 device: str = 'auto'):
        """
        Args:
            model_config: æ¨¡å‹é…ç½®
            device: é‹ç®—è¨­å‚™
        """
        self.model_config = model_config
        self.device = self._setup_device(device)
        self.writer = None  # TensorBoard writer
        
        # å»ºç«‹æ¨¡å‹ - éæ¿¾æ‰ä¸éœ€è¦çš„åƒæ•¸
        valid_model_params = {
            'backbone_name', 'num_classes', 'temporal_frames', 
            'pretrained', 'temporal_fusion', 'dropout', 'freeze_backbone'
        }
        filtered_config = {k: v for k, v in model_config.items() if k in valid_model_params}
        self.model = TemporalFireSmokeClassifier(**filtered_config)
        self.model.to(self.device)
        
        print(f"âœ… å»ºç«‹æ™‚åºæ¨¡å‹: {self.model.backbone_name}")
        print(f"ğŸ’¾ æ¨¡å‹åƒæ•¸: {self.model.get_model_info()}")
        
        # è¨“ç·´ç‹€æ…‹
        self.best_val_acc = 0.0
        self.training_history = {
            'train_loss': [], 'train_acc': [],
            'val_loss': [], 'val_acc': []
        }
    
    def _setup_device(self, device: str) -> torch.device:
        """è¨­ç½®é‹ç®—è¨­å‚™"""
        if device == 'auto':
            if torch.cuda.is_available():
                device = 'cuda'
            else:
                device = 'cpu'
        
        device = torch.device(device)
        print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")
        return device
    
    def prepare_data(self, 
                     dataset_path: str,
                     batch_size: int = 16,
                     val_split: float = 0.2,
                     test_split: float = 0.1,
                     num_workers: int = 4) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """
        æº–å‚™è¨“ç·´ã€é©—è­‰å’Œæ¸¬è©¦è³‡æ–™
        
        Args:
            dataset_path: è³‡æ–™é›†è·¯å¾‘
            batch_size: æ‰¹æ¬¡å¤§å°
            val_split: é©—è­‰é›†æ¯”ä¾‹
            test_split: æ¸¬è©¦é›†æ¯”ä¾‹
            num_workers: è³‡æ–™è¼‰å…¥å·¥ä½œé€²ç¨‹æ•¸
        
        Returns:
            Tuple[DataLoader, DataLoader, DataLoader]: (train_loader, val_loader, test_loader)
        """
        # å»ºç«‹è³‡æ–™é›†
        full_dataset = TemporalFireSmokeDataset(
            dataset_path,
            temporal_frames=self.model_config.get('temporal_frames', 5),
            training=True
        )
        
        # åˆ†å‰²è¨“ç·´ã€é©—è­‰å’Œæ¸¬è©¦é›†
        total_size = len(full_dataset)
        test_size = int(total_size * test_split)
        val_size = int(total_size * val_split)
        train_size = total_size - val_size - test_size
        
        # ç¢ºä¿åˆ†å‰²å¤§å°åˆç†
        if train_size <= 0:
            raise ValueError(f"è¨“ç·´é›†å¤§å°ç‚º {train_size}ï¼Œè«‹èª¿æ•´åˆ†å‰²æ¯”ä¾‹")
        
        train_dataset, val_dataset, test_dataset = random_split(
            full_dataset, 
            [train_size, val_size, test_size],
            generator=torch.Generator().manual_seed(42)
        )
        
        # è¨­å®šé©—è­‰é›†å’Œæ¸¬è©¦é›†ç‚ºéè¨“ç·´æ¨¡å¼
        val_dataset.dataset.training = False  
        test_dataset.dataset.training = False
        
        # å»ºç«‹ DataLoader
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        print(f"ğŸ“Š è³‡æ–™é›†åˆ†å‰²: è¨“ç·´é›† {len(train_dataset)}, é©—è­‰é›† {len(val_dataset)}, æ¸¬è©¦é›† {len(test_dataset)}")
        return train_loader, val_loader, test_loader
    
    def train(self,
              dataset_path: str,
              epochs: int = 50,
              batch_size: int = 16,
              learning_rate: float = 1e-3,
              weight_decay: float = 1e-4,
              val_split: float = 0.2,
              test_split: float = 0.1,
              save_dir: str = 'runs/temporal_training') -> Dict[str, Any]:
        """
        è¨“ç·´æ¨¡å‹
        
        Args:
            dataset_path: è³‡æ–™é›†è·¯å¾‘
            epochs: è¨“ç·´è¼ªæ•¸
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¸ç¿’ç‡
            weight_decay: æ¬Šé‡è¡°æ¸›
            val_split: é©—è­‰é›†æ¯”ä¾‹
            test_split: æ¸¬è©¦é›†æ¯”ä¾‹
            save_dir: å„²å­˜ç›®éŒ„
        
        Returns:
            Dict[str, Any]: è¨“ç·´çµæœ
        """
        # å»ºç«‹å„²å­˜ç›®éŒ„
        save_path = Path(save_dir) / f"temporal_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        save_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ– TensorBoard
        if TENSORBOARD_AVAILABLE:
            self.writer = SummaryWriter(log_dir=str(save_path / 'tensorboard'))
            print(f"ğŸ“Š TensorBoard æ—¥å¿—ä¿å­˜åœ¨: {save_path / 'tensorboard'}")
            
            # è¨˜éŒ„æ¨¡å‹é…ç½®
            self.writer.add_text('Config/Model', str(self.model_config))
            self.writer.add_text('Config/Training', f"""
            - Dataset: {dataset_path}
            - Epochs: {epochs}
            - Batch Size: {batch_size}
            - Learning Rate: {learning_rate}
            - Weight Decay: {weight_decay}
            - Validation Split: {val_split}
            - Test Split: {test_split}
            - Device: {self.device}
            """)
        else:
            print("âš ï¸ TensorBoard ä¸å¯ç”¨ï¼Œè·³éæ—¥å¿—è¨˜éŒ„")
        
        # æº–å‚™è³‡æ–™
        train_loader, val_loader, test_loader = self.prepare_data(
            dataset_path, batch_size, val_split, test_split
        )
        
        # è¨­å®šå„ªåŒ–å™¨å’Œæå¤±å‡½æ•¸
        optimizer = optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=epochs
        )
        
        criterion = nn.CrossEntropyLoss()
        
        # è¨“ç·´è¿´åœˆ
        print(f"ğŸš€ é–‹å§‹è¨“ç·´ {epochs} è¼ª...")
        
        for epoch in range(epochs):
            # è¨“ç·´éšæ®µ
            train_loss, train_acc = self._train_epoch(
                train_loader, optimizer, criterion
            )
            
            # é©—è­‰éšæ®µ
            val_loss, val_acc = self._validate_epoch(
                val_loader, criterion
            )
            
            # æ›´æ–°å­¸ç¿’ç‡
            scheduler.step()
            
            # è¨˜éŒ„æ­·å²
            self.training_history['train_loss'].append(train_loss)
            self.training_history['train_acc'].append(train_acc)
            self.training_history['val_loss'].append(val_loss)
            self.training_history['val_acc'].append(val_acc)
            
            # è¨˜éŒ„åˆ° TensorBoard
            if self.writer:
                self.writer.add_scalar('Loss/Train', train_loss, epoch)
                self.writer.add_scalar('Loss/Validation', val_loss, epoch)
                self.writer.add_scalar('Accuracy/Train', train_acc, epoch)
                self.writer.add_scalar('Accuracy/Validation', val_acc, epoch)
                self.writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)
                
                # è¨˜éŒ„æ¨¡å‹åƒæ•¸åˆ†ä½ˆ (æ¯10å€‹epochè¨˜éŒ„ä¸€æ¬¡)
                if epoch % 10 == 0:
                    for name, param in self.model.named_parameters():
                        if param.grad is not None:
                            self.writer.add_histogram(f'Parameters/{name}', param, epoch)
                            self.writer.add_histogram(f'Gradients/{name}', param.grad, epoch)
            
            # å„²å­˜æœ€ä½³æ¨¡å‹
            is_best = val_acc > self.best_val_acc
            if is_best:
                self.best_val_acc = val_acc
                self._save_model(save_path / 'best_model.pth')
                
                # åœ¨ TensorBoard ä¸­æ¨™è¨˜æœ€ä½³æº–ç¢ºç‡
                if self.writer:
                    self.writer.add_scalar('Best/Validation_Accuracy', val_acc, epoch)
            
            # å°å‡ºé€²åº¦
            print(f"Epoch {epoch+1}/{epochs}: "
                  f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
                  f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}"
                  f"{' ğŸŒŸ (New Best!)' if is_best else ''}")
        
        # åœ¨æœ€ä½³æ¨¡å‹ä¸Šé€²è¡Œæ¸¬è©¦é›†è©•ä¼°
        print(f"ğŸ§ª åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°æœ€ä½³æ¨¡å‹...")
        self._load_model(save_path / 'best_model.pth')
        test_loss, test_acc = self._validate_epoch(test_loader, criterion)
        print(f"ğŸ“Š æ¸¬è©¦é›†çµæœ: Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}")
        
        # å„²å­˜æœ€çµ‚æ¨¡å‹å’Œè¨“ç·´æ­·å²
        self._save_model(save_path / 'final_model.pth')
        self._save_training_history(save_path)
        self._plot_training_curves(save_path)
        
        # è¨˜éŒ„æœ€çµ‚çµæœåˆ° TensorBoard
        if self.writer:
            # è¨˜éŒ„æœ€çµ‚æº–ç¢ºç‡
            self.writer.add_scalar('Final/Best_Validation_Accuracy', self.best_val_acc, epochs)
            self.writer.add_scalar('Final/Final_Train_Accuracy', self.training_history['train_acc'][-1], epochs)
            self.writer.add_scalar('Final/Test_Accuracy', test_acc, epochs)
            self.writer.add_scalar('Final/Test_Loss', test_loss, epochs)
            
            # è¨˜éŒ„è¨“ç·´æ›²ç·šåœ–ç‰‡ (å¦‚æœå­˜åœ¨)
            curves_path = save_path / 'training_curves.png'
            if curves_path.exists():
                try:
                    import matplotlib.image as mpimg
                    img = mpimg.imread(str(curves_path))
                    self.writer.add_image('Training_Curves', img, 0, dataformats='HWC')
                except Exception as e:
                    print(f"âš ï¸ ç„¡æ³•æ·»åŠ è¨“ç·´æ›²ç·šåœ–ç‰‡åˆ° TensorBoard: {e}")
            
            # è¨˜éŒ„è¶…åƒæ•¸
            hparams = {
                'backbone': self.model_config.get('backbone_name', 'unknown'),
                'learning_rate': learning_rate,
                'batch_size': batch_size,
                'epochs': epochs,
                'weight_decay': weight_decay,
                'temporal_frames': self.model_config.get('temporal_frames', 5),
            }
            metrics = {
                'best_val_accuracy': self.best_val_acc,
                'final_train_accuracy': self.training_history['train_acc'][-1],
                'test_accuracy': test_acc,
                'test_loss': test_loss,
            }
            self.writer.add_hparams(hparams, metrics)
            
            # é—œé–‰ writer
            self.writer.close()
            print(f"ğŸ“Š TensorBoard æ—¥å¿—å·²ä¿å­˜ï¼Œå¯ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹:")
            print(f"   tensorboard --logdir={save_path / 'tensorboard'}")
        
        result = {
            'best_val_accuracy': self.best_val_acc,
            'final_train_accuracy': self.training_history['train_acc'][-1],
            'test_accuracy': test_acc,
            'test_loss': test_loss,
            'save_path': str(save_path),
            'tensorboard_path': str(save_path / 'tensorboard') if TENSORBOARD_AVAILABLE else None,
            'model_info': self.model.get_model_info()
        }
        
        print(f"âœ… è¨“ç·´å®Œæˆ! æœ€ä½³é©—è­‰æº–ç¢ºç‡: {self.best_val_acc:.4f}")
        return result
    
    def _train_epoch(self, train_loader: DataLoader, 
                     optimizer: optim.Optimizer, 
                     criterion: nn.Module) -> Tuple[float, float]:
        """è¨“ç·´ä¸€å€‹ epoch"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc="Training")
        for frames, labels in pbar:
            frames = frames.to(self.device)
            labels = labels.to(self.device)
            
            optimizer.zero_grad()
            outputs = self.model(frames)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # æ›´æ–°é€²åº¦æ¢
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
        
        return total_loss / len(train_loader), correct / total
    
    def _validate_epoch(self, val_loader: DataLoader, 
                        criterion: nn.Module) -> Tuple[float, float]:
        """é©—è­‰ä¸€å€‹ epoch"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for frames, labels in val_loader:
                frames = frames.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(frames)
                loss = criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        return total_loss / len(val_loader), correct / total
    
    def _save_model(self, path: Path):
        """å„²å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': self.model_config,
            'best_val_acc': self.best_val_acc,
            'training_history': self.training_history
        }, path)
    
    def _load_model(self, path: Path):
        """è¼‰å…¥æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
    
    def _save_training_history(self, save_dir: Path):
        """å„²å­˜è¨“ç·´æ­·å²"""
        history_path = save_dir / 'training_history.json'
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)
    
    def _plot_training_curves(self, save_dir: Path):
        """ç¹ªè£½è¨“ç·´æ›²ç·š"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # Loss æ›²ç·š
        ax1.plot(self.training_history['train_loss'], label='Train Loss')
        ax1.plot(self.training_history['val_loss'], label='Val Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # Accuracy æ›²ç·š
        ax2.plot(self.training_history['train_acc'], label='Train Acc')
        ax2.plot(self.training_history['val_acc'], label='Val Acc')
        ax2.set_title('Training and Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(save_dir / 'training_curves.png', dpi=300, bbox_inches='tight')
        plt.close()


def load_temporal_model(model_path: str, device: str = 'auto') -> TemporalFireSmokeClassifier:
    """
    è¼‰å…¥å·²è¨“ç·´çš„æ™‚åºæ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹è·¯å¾‘
        device: é‹ç®—è¨­å‚™
    
    Returns:
        TemporalFireSmokeClassifier: è¼‰å…¥çš„æ¨¡å‹
    """
    # è¨­å®šè¨­å‚™
    if device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    device = torch.device(device)
    
    # è¼‰å…¥æª¢æŸ¥é»
    checkpoint = torch.load(model_path, map_location=device)
    
    # å»ºç«‹æ¨¡å‹ - éæ¿¾æ‰ä¸æ”¯æ´çš„åƒæ•¸
    model_config = checkpoint['model_config']
    valid_params = {
        'backbone_name', 'num_classes', 'temporal_frames',
        'pretrained', 'temporal_fusion', 'dropout', 'freeze_backbone'
    }
    filtered_config = {k: v for k, v in model_config.items() if k in valid_params}
    
    model = TemporalFireSmokeClassifier(**filtered_config)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    print(f"âœ… è¼‰å…¥æ™‚åºæ¨¡å‹: {model.backbone_name}")
    return model
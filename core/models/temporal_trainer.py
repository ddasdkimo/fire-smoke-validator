#!/usr/bin/env python3
"""
æ™‚åºæ¨¡å‹è¨“ç·´å™¨
è™•ç†æ™‚åºè³‡æ–™çš„è¼‰å…¥ã€è¨“ç·´å’Œè©•ä¼°
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import cv2
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from datetime import datetime

from .temporal_classifier import TemporalFireSmokeClassifier, DEFAULT_MODEL_CONFIGS
from .data_utils import prepare_temporal_frames, create_temporal_batch


class TemporalFireSmokeDataset(Dataset):
    """
    æ™‚åºç«ç…™è³‡æ–™é›†
    å¾æ¨™è¨»çš„äº‹ä»¶è³‡æ–™è¼‰å…¥æ™‚åºå¹€åºåˆ—
    """
    
    def __init__(self, 
                 dataset_path: str,
                 temporal_frames: int = 5,
                 image_size: Tuple[int, int] = (224, 224),
                 training: bool = True):
        """
        Args:
            dataset_path: è³‡æ–™é›†è·¯å¾‘
            temporal_frames: æ™‚åºå¹€æ•¸
            image_size: å½±åƒå°ºå¯¸
            training: æ˜¯å¦ç‚ºè¨“ç·´æ¨¡å¼
        """
        self.dataset_path = Path(dataset_path)
        self.temporal_frames = temporal_frames
        self.image_size = image_size
        self.training = training
        
        # è¼‰å…¥è³‡æ–™
        self.samples = self._load_dataset()
        
        # é¡åˆ¥æ˜ å°„
        self.class_to_idx = {'false_positive': 0, 'true_positive': 1}
        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}
        
        print(f"âœ… è¼‰å…¥ {len(self.samples)} å€‹æ™‚åºæ¨£æœ¬")
        self._print_dataset_stats()
    
    def _load_dataset(self) -> List[Dict[str, Any]]:
        """è¼‰å…¥è³‡æ–™é›†æ¨£æœ¬"""
        samples = []
        
        for class_name in ['true_positive', 'false_positive']:
            class_dir = self.dataset_path / class_name
            if not class_dir.exists():
                continue
                
            for event_dir in class_dir.iterdir():
                if not event_dir.is_dir():
                    continue
                
                # æŸ¥æ‰¾äº‹ä»¶ä¸­çš„æ‰€æœ‰å½±åƒ
                image_files = []
                for ext in ['*.jpg', '*.jpeg', '*.png']:
                    image_files.extend(list(event_dir.glob(ext)))
                
                if len(image_files) == 0:
                    continue
                
                # æŒ‰æª”åæ’åºç¢ºä¿æ™‚åºæ­£ç¢º
                image_files.sort(key=lambda x: x.name)
                
                # è¼‰å…¥ metadata (å¦‚æœå­˜åœ¨)
                metadata_path = event_dir / 'metadata.json'
                metadata = {}
                if metadata_path.exists():
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                    except:
                        pass
                
                samples.append({
                    'event_dir': str(event_dir),
                    'image_files': [str(f) for f in image_files],
                    'class_name': class_name,
                    'label': self.class_to_idx[class_name],
                    'metadata': metadata
                })
        
        return samples
    
    def _print_dataset_stats(self):
        """å°å‡ºè³‡æ–™é›†çµ±è¨ˆ"""
        stats = {}
        for sample in self.samples:
            class_name = sample['class_name']
            if class_name not in stats:
                stats[class_name] = {'count': 0, 'total_frames': 0}
            stats[class_name]['count'] += 1
            stats[class_name]['total_frames'] += len(sample['image_files'])
        
        print("ğŸ“Š è³‡æ–™é›†çµ±è¨ˆ:")
        for class_name, stat in stats.items():
            avg_frames = stat['total_frames'] / stat['count']
            print(f"  {class_name}: {stat['count']} äº‹ä»¶, å¹³å‡ {avg_frames:.1f} å¹€")
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        """
        ç²å–å–®å€‹æ¨£æœ¬
        
        Returns:
            Tuple[torch.Tensor, int]: (frames [T, C, H, W], label)
        """
        sample = self.samples[idx]
        
        # è¼‰å…¥æ‰€æœ‰å¹€
        frames = []
        for image_path in sample['image_files']:
            frame = cv2.imread(image_path)
            if frame is not None:
                frames.append(frame)
        
        if not frames:
            # å¦‚æœç„¡æ³•è¼‰å…¥ä»»ä½•å¹€ï¼Œè¿”å›é»‘è‰²å¹€
            frames = [np.zeros((224, 224, 3), dtype=np.uint8)]
        
        # æº–å‚™æ™‚åºå¹€åºåˆ—
        temporal_frames = prepare_temporal_frames(
            frames, 
            target_frames=self.temporal_frames,
            image_size=self.image_size,
            training=self.training
        )
        
        return temporal_frames, sample['label']


class TemporalTrainer:
    """
    æ™‚åºæ¨¡å‹è¨“ç·´å™¨
    """
    
    def __init__(self, 
                 model_config: Dict[str, Any],
                 device: str = 'auto'):
        """
        Args:
            model_config: æ¨¡å‹é…ç½®
            device: é‹ç®—è¨­å‚™
        """
        self.model_config = model_config
        self.device = self._setup_device(device)
        
        # å»ºç«‹æ¨¡å‹
        self.model = TemporalFireSmokeClassifier(**model_config)
        self.model.to(self.device)
        
        print(f"âœ… å»ºç«‹æ™‚åºæ¨¡å‹: {self.model.backbone_name}")
        print(f"ğŸ’¾ æ¨¡å‹åƒæ•¸: {self.model.get_model_info()}")
        
        # è¨“ç·´ç‹€æ…‹
        self.best_val_acc = 0.0
        self.training_history = {
            'train_loss': [], 'train_acc': [],
            'val_loss': [], 'val_acc': []
        }
    
    def _setup_device(self, device: str) -> torch.device:
        """è¨­ç½®é‹ç®—è¨­å‚™"""
        if device == 'auto':
            if torch.cuda.is_available():
                device = 'cuda'
            else:
                device = 'cpu'
        
        device = torch.device(device)
        print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")
        return device
    
    def prepare_data(self, 
                     dataset_path: str,
                     batch_size: int = 16,
                     val_split: float = 0.2,
                     num_workers: int = 4) -> Tuple[DataLoader, DataLoader]:
        """
        æº–å‚™è¨“ç·´å’Œé©—è­‰è³‡æ–™
        
        Args:
            dataset_path: è³‡æ–™é›†è·¯å¾‘
            batch_size: æ‰¹æ¬¡å¤§å°
            val_split: é©—è­‰é›†æ¯”ä¾‹
            num_workers: è³‡æ–™è¼‰å…¥å·¥ä½œé€²ç¨‹æ•¸
        
        Returns:
            Tuple[DataLoader, DataLoader]: (train_loader, val_loader)
        """
        # å»ºç«‹è³‡æ–™é›†
        full_dataset = TemporalFireSmokeDataset(
            dataset_path,
            temporal_frames=self.model_config.get('temporal_frames', 5),
            training=True
        )
        
        # åˆ†å‰²è¨“ç·´å’Œé©—è­‰é›†
        val_size = int(len(full_dataset) * val_split)
        train_size = len(full_dataset) - val_size
        
        train_dataset, val_dataset = random_split(
            full_dataset, 
            [train_size, val_size],
            generator=torch.Generator().manual_seed(42)
        )
        
        # è¨­å®šé©—è­‰é›†ç‚ºéè¨“ç·´æ¨¡å¼
        val_dataset.dataset.training = False
        
        # å»ºç«‹ DataLoader
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        print(f"ğŸ“Š è³‡æ–™åˆ†å‰²: è¨“ç·´ {len(train_dataset)}, é©—è­‰ {len(val_dataset)}")
        
        return train_loader, val_loader
    
    def train(self,
              dataset_path: str,
              epochs: int = 50,
              batch_size: int = 16,
              learning_rate: float = 1e-3,
              weight_decay: float = 1e-4,
              val_split: float = 0.2,
              save_dir: str = 'runs/temporal_training') -> Dict[str, Any]:
        """
        è¨“ç·´æ¨¡å‹
        
        Args:
            dataset_path: è³‡æ–™é›†è·¯å¾‘
            epochs: è¨“ç·´è¼ªæ•¸
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¸ç¿’ç‡
            weight_decay: æ¬Šé‡è¡°æ¸›
            val_split: é©—è­‰é›†æ¯”ä¾‹
            save_dir: å„²å­˜ç›®éŒ„
        
        Returns:
            Dict[str, Any]: è¨“ç·´çµæœ
        """
        # å»ºç«‹å„²å­˜ç›®éŒ„
        save_path = Path(save_dir) / f"temporal_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        save_path.mkdir(parents=True, exist_ok=True)
        
        # æº–å‚™è³‡æ–™
        train_loader, val_loader = self.prepare_data(
            dataset_path, batch_size, val_split
        )
        
        # è¨­å®šå„ªåŒ–å™¨å’Œæå¤±å‡½æ•¸
        optimizer = optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=epochs
        )
        
        criterion = nn.CrossEntropyLoss()
        
        # è¨“ç·´è¿´åœˆ
        print(f"ğŸš€ é–‹å§‹è¨“ç·´ {epochs} è¼ª...")
        
        for epoch in range(epochs):
            # è¨“ç·´éšæ®µ
            train_loss, train_acc = self._train_epoch(
                train_loader, optimizer, criterion
            )
            
            # é©—è­‰éšæ®µ
            val_loss, val_acc = self._validate_epoch(
                val_loader, criterion
            )
            
            # æ›´æ–°å­¸ç¿’ç‡
            scheduler.step()
            
            # è¨˜éŒ„æ­·å²
            self.training_history['train_loss'].append(train_loss)
            self.training_history['train_acc'].append(train_acc)
            self.training_history['val_loss'].append(val_loss)
            self.training_history['val_acc'].append(val_acc)
            
            # å„²å­˜æœ€ä½³æ¨¡å‹
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self._save_model(save_path / 'best_model.pth')
            
            # å°å‡ºé€²åº¦
            print(f"Epoch {epoch+1}/{epochs}: "
                  f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
                  f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
        
        # å„²å­˜æœ€çµ‚æ¨¡å‹å’Œè¨“ç·´æ­·å²
        self._save_model(save_path / 'final_model.pth')
        self._save_training_history(save_path)
        self._plot_training_curves(save_path)
        
        result = {
            'best_val_accuracy': self.best_val_acc,
            'final_train_accuracy': self.training_history['train_acc'][-1],
            'save_path': str(save_path),
            'model_info': self.model.get_model_info()
        }
        
        print(f"âœ… è¨“ç·´å®Œæˆ! æœ€ä½³é©—è­‰æº–ç¢ºç‡: {self.best_val_acc:.4f}")
        return result
    
    def _train_epoch(self, train_loader: DataLoader, 
                     optimizer: optim.Optimizer, 
                     criterion: nn.Module) -> Tuple[float, float]:
        """è¨“ç·´ä¸€å€‹ epoch"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc="Training")
        for frames, labels in pbar:
            frames = frames.to(self.device)
            labels = labels.to(self.device)
            
            optimizer.zero_grad()
            outputs = self.model(frames)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # æ›´æ–°é€²åº¦æ¢
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
        
        return total_loss / len(train_loader), correct / total
    
    def _validate_epoch(self, val_loader: DataLoader, 
                        criterion: nn.Module) -> Tuple[float, float]:
        """é©—è­‰ä¸€å€‹ epoch"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for frames, labels in val_loader:
                frames = frames.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(frames)
                loss = criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        return total_loss / len(val_loader), correct / total
    
    def _save_model(self, path: Path):
        """å„²å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': self.model_config,
            'best_val_acc': self.best_val_acc,
            'training_history': self.training_history
        }, path)
    
    def _save_training_history(self, save_dir: Path):
        """å„²å­˜è¨“ç·´æ­·å²"""
        history_path = save_dir / 'training_history.json'
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)
    
    def _plot_training_curves(self, save_dir: Path):
        """ç¹ªè£½è¨“ç·´æ›²ç·š"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # Loss æ›²ç·š
        ax1.plot(self.training_history['train_loss'], label='Train Loss')
        ax1.plot(self.training_history['val_loss'], label='Val Loss')
        ax1.set_title('Training and Validation Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # Accuracy æ›²ç·š
        ax2.plot(self.training_history['train_acc'], label='Train Acc')
        ax2.plot(self.training_history['val_acc'], label='Val Acc')
        ax2.set_title('Training and Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig(save_dir / 'training_curves.png', dpi=300, bbox_inches='tight')
        plt.close()


def load_temporal_model(model_path: str, device: str = 'auto') -> TemporalFireSmokeClassifier:
    """
    è¼‰å…¥å·²è¨“ç·´çš„æ™‚åºæ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹è·¯å¾‘
        device: é‹ç®—è¨­å‚™
    
    Returns:
        TemporalFireSmokeClassifier: è¼‰å…¥çš„æ¨¡å‹
    """
    # è¨­å®šè¨­å‚™
    if device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    device = torch.device(device)
    
    # è¼‰å…¥æª¢æŸ¥é»
    checkpoint = torch.load(model_path, map_location=device)
    
    # å»ºç«‹æ¨¡å‹
    model = TemporalFireSmokeClassifier(**checkpoint['model_config'])
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    print(f"âœ… è¼‰å…¥æ™‚åºæ¨¡å‹: {model.backbone_name}")
    return model
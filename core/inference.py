#!/usr/bin/env python3
"""
æ¨è«–æ¨¡çµ„
è² è²¬æ¨¡å‹æ¨è«–åŠŸèƒ½
"""

import cv2
import numpy as np
from pathlib import Path
import json
import tempfile
import os
from datetime import datetime

try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

try:
    import torch
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import matplotlib.pyplot as plt
    import matplotlib.cm as cm
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False

try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False


class GradCAMVisualizer:
    """Grad-CAM è¦–è¦ºåŒ–å·¥å…·"""
    
    def __init__(self, model):
        self.model = model
        self.target_layers = []
        self.gradients = []
        self.activations = []
        
        # è¨»å†Š hook åˆ°ç›®æ¨™å±¤
        self._register_hooks()
    
    def _register_hooks(self):
        """è¨»å†Š forward å’Œ backward hooks"""
        # æ‰¾åˆ°backboneçš„æœ€å¾Œä¸€å€‹å·ç©å±¤
        target_layer = None
        
        # éæ­·æ¨¡å‹å°‹æ‰¾åˆé©çš„ç‰¹å¾µå±¤
        for name, module in self.model.named_modules():
            if 'backbone' in name and hasattr(module, 'weight') and len(module.weight.shape) == 4:
                target_layer = module
        
        if target_layer is not None:
            # è¨»å†Š forward hook
            target_layer.register_forward_hook(self._forward_hook)
            # è¨»å†Š backward hook
            target_layer.register_full_backward_hook(self._backward_hook)
    
    def _forward_hook(self, module, input, output):
        """Forward hook å„²å­˜æ¿€æ´»å€¼"""
        self.activations = output.detach()
    
    def _backward_hook(self, module, grad_input, grad_output):
        """Backward hook å„²å­˜æ¢¯åº¦"""
        self.gradients = grad_output[0].detach()
    
    def generate_cam(self, input_tensor, target_class=None):
        """ç”Ÿæˆ Class Activation Map"""
        try:
            self.model.eval()
            
            # Forward pass
            output = self.model(input_tensor)
            
            if target_class is None:
                target_class = torch.argmax(output, dim=1)
            
            # Backward pass
            self.model.zero_grad()
            class_loss = output[0, target_class]
            class_loss.backward(retain_graph=True)
            
            # è¨ˆç®— Grad-CAM
            if len(self.gradients) > 0 and len(self.activations) > 0:
                # æ± åŒ–æ¢¯åº¦å¾—åˆ°æ¬Šé‡
                weights = torch.mean(self.gradients, dim=[2, 3], keepdim=True)
                
                # åŠ æ¬Šçµ„åˆç‰¹å¾µåœ–
                cam = torch.sum(weights * self.activations, dim=1, keepdim=True)
                cam = F.relu(cam)  # ReLU activation
                
                # æ­£æ­¸åŒ–åˆ° 0-1
                cam = cam - cam.min()
                cam = cam / (cam.max() + 1e-8)
                
                return cam.squeeze().cpu().numpy()
            
        except Exception as e:
            print(f"ç”Ÿæˆ Grad-CAM æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
        return None


class AttentionVisualizer:
    """æ³¨æ„åŠ›æ¬Šé‡è¦–è¦ºåŒ–å·¥å…·"""
    
    def __init__(self, model):
        self.model = model
        self.attention_weights = {}
        
        # è¨»å†Š hook åˆ°æ³¨æ„åŠ›å±¤
        self._register_attention_hooks()
    
    def _register_attention_hooks(self):
        """è¨»å†Šæ³¨æ„åŠ›å±¤çš„ hooks"""
        for name, module in self.model.named_modules():
            # å°‹æ‰¾æ³¨æ„åŠ›æ¨¡çµ„
            if 'attention' in name.lower() or 'attn' in name.lower():
                module.register_forward_hook(
                    lambda module, input, output, name=name: self._attention_hook(name, output)
                )
    
    def _attention_hook(self, name, output):
        """å„²å­˜æ³¨æ„åŠ›æ¬Šé‡"""
        if isinstance(output, tuple):
            # é€šå¸¸æ³¨æ„åŠ›æ¨¡çµ„æœƒè¿”å› (output, attention_weights)
            if len(output) > 1:
                self.attention_weights[name] = output[1].detach().cpu()
        elif hasattr(output, 'attention_weights'):
            self.attention_weights[name] = output.attention_weights.detach().cpu()
    
    def get_attention_maps(self):
        """å–å¾—æ³¨æ„åŠ›åœ°åœ–"""
        return self.attention_weights


class ModelInference:
    """æ¨¡å‹æ¨è«–å™¨"""
    
    def __init__(self):
        self.inference_dir = Path("inference_workspace")
        self.inference_dir.mkdir(exist_ok=True)
        
        # ç•¶å‰è¼‰å…¥çš„æ¨¡å‹
        self.current_model = None
        self.current_model_path = None
        self.current_model_info = ""
        
        # æ¨è«–çµæœ
        self.inference_results = []
        
        # è¦–è¦ºåŒ–å·¥å…·
        self.gradcam_visualizer = None
        self.attention_visualizer = None
    
    def get_available_models(self):
        """å–å¾—å¯ç”¨çš„æ™‚åºæ¨¡å‹åˆ—è¡¨"""
        models = []
        
        # æ™‚åºè¨“ç·´æ¨¡å‹ - å¾ runs/temporal_training ç›®éŒ„
        temporal_runs_dir = Path("runs/temporal_training")
        if temporal_runs_dir.exists():
            for run_dir in temporal_runs_dir.iterdir():
                if run_dir.is_dir():
                    # å°‹æ‰¾ best_model.pth
                    best_model = run_dir / "best_model.pth"
                    if best_model.exists():
                        models.append({
                            "name": f"æ™‚åºæ¨¡å‹ - {run_dir.name}",
                            "path": str(best_model),
                            "type": "temporal_best",
                            "created": best_model.stat().st_mtime
                        })
                    
                    # å°‹æ‰¾ final_model.pth
                    final_model = run_dir / "final_model.pth"
                    if final_model.exists():
                        models.append({
                            "name": f"æ™‚åºæ¨¡å‹ (æœ€çµ‚) - {run_dir.name}",
                            "path": str(final_model),
                            "type": "temporal_final",
                            "created": final_model.stat().st_mtime
                        })
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹ï¼Œæä¾›é è¨­èªªæ˜
        if not models:
            models.append({
                "name": "å°šç„¡å·²è¨“ç·´æ¨¡å‹ (è«‹å…ˆé€²è¡Œè¨“ç·´)",
                "path": "",
                "type": "placeholder",
                "created": 0
            })
        
        # æŒ‰æ™‚é–“æ’åºï¼Œæœ€æ–°çš„åœ¨å‰é¢
        models.sort(key=lambda x: -x.get("created", 0))
        return models
    
    def load_model(self, model_path, device='auto'):
        """è¼‰å…¥æ™‚åºæ¨è«–æ¨¡å‹"""
        try:
            if not model_path or model_path == "":
                return "âŒ è«‹å…ˆé¸æ“‡æœ‰æ•ˆçš„æ¨¡å‹"
            
            if not Path(model_path).exists():
                return f"âŒ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {model_path}"
            
            # è¼‰å…¥æ™‚åºæ¨¡å‹
            return self._load_temporal_model(model_path, device)
            
        except Exception as e:
            self.current_model = None
            self.current_model_path = None
            return f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}"
    
    
    def _load_temporal_model(self, model_path, device):
        """è¼‰å…¥æ™‚åºæ¨¡å‹"""
        try:
            if not TORCH_AVAILABLE:
                return "âŒ PyTorch æœªå®‰è£ï¼Œç„¡æ³•è¼‰å…¥æ™‚åºæ¨¡å‹"
                
            from .models.temporal_trainer import load_temporal_model
            
            # è¨­å®šè¨­å‚™
            if device == 'auto':
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            self.current_model = load_temporal_model(model_path, device)
            self.current_model_path = model_path
            
            # åˆå§‹åŒ–è¦–è¦ºåŒ–å·¥å…·
            self.gradcam_visualizer = GradCAMVisualizer(self.current_model)
            self.attention_visualizer = AttentionVisualizer(self.current_model)
            
            # å–å¾—æ¨¡å‹è³‡è¨Š
            model_info = self.current_model.get_model_info()
            self.current_model_info = f"""âœ… æ™‚åºæ¨¡å‹è¼‰å…¥æˆåŠŸï¼

ğŸ“ æ¨¡å‹è·¯å¾‘: {model_path}
âš¡ é‹ç®—è¨­å‚™: {device}
ğŸ¯ æ¨¡å‹é¡å‹: æ™‚åºç«ç…™åˆ†é¡å™¨
ğŸ§  Backbone: {model_info['backbone']}
â±ï¸ æ™‚åºå¹€æ•¸: {model_info['temporal_frames']}
ğŸ”„ èåˆç­–ç•¥: {model_info['temporal_fusion']}
ğŸ“Š åƒæ•¸é‡: {model_info['total_parameters']:,}
ğŸƒ å¯è¨“ç·´åƒæ•¸: {model_info['trainable_parameters']:,}
            """
            
            return self.current_model_info
            
        except ImportError:
            return "âŒ ç¼ºå°‘ timm æˆ–ç›¸é—œä¾è³´ï¼Œç„¡æ³•è¼‰å…¥æ™‚åºæ¨¡å‹"
        except Exception as e:
            return f"âŒ æ™‚åºæ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}"
    
    
    def _get_model_info(self, model_path):
        """å–å¾—æ¨¡å‹è©³ç´°è³‡è¨Š"""
        try:
            # åŸºæœ¬è³‡è¨Š
            file_size = Path(model_path).stat().st_size / (1024 * 1024)  # MB
            info = f"ğŸ“Š æª”æ¡ˆå¤§å°: {file_size:.1f} MB\n"
            
            # å¦‚æœæœ‰æ¨¡å‹çš„è©±ï¼Œå¯ä»¥å–å¾—æ›´å¤šè³‡è¨Š
            if self.current_model:
                # é¡åˆ¥æ•¸é‡
                if hasattr(self.current_model.model, 'nc'):
                    info += f"ğŸ¯ é¡åˆ¥æ•¸é‡: {self.current_model.model.nc}\n"
                
                # é¡åˆ¥åç¨±
                if hasattr(self.current_model.model, 'names'):
                    names = self.current_model.model.names
                    info += f"ğŸ“ é¡åˆ¥åç¨±: {list(names.values())}\n"
            
            return info
            
        except Exception as e:
            return f"âš ï¸ ç„¡æ³•å–å¾—è©³ç´°è³‡è¨Š: {str(e)}\n"
    
    def inference_batch_images(self, image_files, confidence_threshold=0.5):
        """æ™‚åºæ¨¡å‹æ‰¹æ¬¡æ¨è«–"""
        try:
            if not self.current_model:
                return "âŒ è«‹å…ˆè¼‰å…¥æ™‚åºæ¨¡å‹", []
            
            if not image_files:
                return "âŒ è«‹ä¸Šå‚³æ™‚åºå½±åƒæª”æ¡ˆï¼ˆå»ºè­°ä¸Šå‚³åŒä¸€äº‹ä»¶çš„å¤šå¹€å½±åƒï¼‰", []
            
            # åŸ·è¡Œæ™‚åºæ¨¡å‹æ¨è«–
            return self._inference_temporal_model(image_files, confidence_threshold)
            
        except Exception as e:
            return f"âŒ æ™‚åºæ¨è«–å¤±æ•—: {str(e)}", []
    
    def _inference_temporal_model(self, image_files, confidence_threshold=0.5):
        """æ™‚åºæ¨¡å‹æ¨è«–"""
        try:
            if not TORCH_AVAILABLE:
                return "âŒ PyTorch æœªå®‰è£ï¼Œç„¡æ³•é€²è¡Œæ™‚åºæ¨è«–", []
                
            from .models.data_utils import prepare_temporal_frames
            
            results = []
            processed_count = 0
            
            # ç¢ºä¿æ˜¯åˆ—è¡¨æ ¼å¼
            if not isinstance(image_files, list):
                image_files = [image_files]
            
            # æ™‚åºæ¨¡å‹éœ€è¦å°‡å¤šå¼µå½±åƒä½œç‚ºä¸€å€‹åºåˆ—è™•ç†
            # é€™è£¡å‡è¨­ç”¨æˆ¶ä¸Šå‚³çš„æ˜¯ä¸€å€‹äº‹ä»¶çš„å¤šå€‹å¹€
            print(f"â±ï¸ æ™‚åºæ¨¡å‹æ¨è«–: è™•ç† {len(image_files)} å¼µå½±åƒä½œç‚ºä¸€å€‹æ™‚åºåºåˆ—...")
            
            try:
                # è¼‰å…¥æ‰€æœ‰å½±åƒä½œç‚ºä¸€å€‹æ™‚åºåºåˆ—
                frames = []
                valid_files = []
                
                for img_file in image_files:
                    image = cv2.imread(img_file.name)
                    if image is not None:
                        frames.append(image)
                        valid_files.append(img_file)
                
                if not frames:
                    return "âŒ ç„¡æ³•è®€å–ä»»ä½•æœ‰æ•ˆå½±åƒ", []
                
                # æº–å‚™æ™‚åºè¼¸å…¥ (T=5)
                temporal_input = prepare_temporal_frames(frames, target_frames=5, training=False)
                temporal_input = temporal_input.unsqueeze(0)  # [1, T, C, H, W] åŠ å…¥ batch ç¶­åº¦
                
                # ç§»å‹•åˆ°æ­£ç¢ºè¨­å‚™
                device = next(self.current_model.parameters()).device
                temporal_input = temporal_input.to(device)
                
                # é€²è¡Œæ¨è«–
                with torch.no_grad():
                    outputs = self.current_model(temporal_input)  # [1, num_classes]
                    probabilities = torch.softmax(outputs, dim=1)  # è½‰æ›ç‚ºæ©Ÿç‡
                    
                    predicted_class = torch.argmax(probabilities, dim=1).item()
                    confidence = probabilities[0][predicted_class].item()
                
                # ç”Ÿæˆç†±å€åœ– (éœ€è¦æ¢¯åº¦ï¼Œæ‰€ä»¥é‡æ–°è¨ˆç®—)
                heatmaps = self._generate_heatmaps(temporal_input, predicted_class)
                
                # é¡åˆ¥æ˜ å°„
                class_names = {0: "false_positive", 1: "true_positive"}
                predicted_label = class_names.get(predicted_class, f"class_{predicted_class}")
                
                # å»ºç«‹çµæœåœ–åƒï¼ˆå°‡æ‰€æœ‰å¹€çµ„åˆæˆç¶²æ ¼ï¼‰
                grid_image = self._create_temporal_result_grid(frames, predicted_label, confidence)
                
                # å„²å­˜çµæœåœ–åƒ
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_filename = f"temporal_inference_{timestamp}.jpg"
                output_path = self.inference_dir / output_filename
                cv2.imwrite(str(output_path), grid_image)
                
                # ç”Ÿæˆå’Œå„²å­˜ç†±å€åœ–è¦–è¦ºåŒ–
                heatmap_paths = self._save_heatmap_visualizations(
                    frames, heatmaps, predicted_label, confidence, timestamp
                )
                
                sequence_result = {
                    "sequence_id": f"temporal_sequence_{timestamp}",
                    "input_frames": [Path(f.name).name for f in valid_files],
                    "predicted_class": predicted_class,
                    "predicted_label": predicted_label,
                    "confidence": float(confidence),
                    "probabilities": {
                        "false_positive": float(probabilities[0][0]),
                        "true_positive": float(probabilities[0][1])
                    },
                    "result_image_path": str(output_path),
                    "heatmap_paths": heatmap_paths,  # æ–°å¢ç†±å€åœ–è·¯å¾‘
                    "has_heatmaps": len(heatmap_paths) > 0,  # æ¨™è¨˜æ˜¯å¦æœ‰ç†±å€åœ–
                    "total_frames": len(frames),
                    "processed_frames": 5  # å›ºå®šè™•ç†5å¹€
                }
                
                results.append(sequence_result)
                
                # ç”Ÿæˆè©³ç´°æ‘˜è¦
                status_emoji = "ğŸ”¥" if predicted_label == "true_positive" else "âœ…"
                result_name = "çœŸå¯¦ç«ç…™äº‹ä»¶" if predicted_label == "true_positive" else "éç«ç…™äº‹ä»¶"
                
                # ç†±å€åœ–ç‹€æ…‹
                heatmap_status = "âœ… å·²ç”Ÿæˆ" if len(heatmap_paths) > 0 else "âŒ æœªç”Ÿæˆ"
                heatmap_info = f"- ğŸ”¥ ç†±å€åœ–: {heatmap_status} ({len(heatmap_paths)} å€‹æª”æ¡ˆ)" if len(heatmap_paths) > 0 else "- ğŸ”¥ ç†±å€åœ–: ç”Ÿæˆå¤±æ•—æˆ–ä¸æ”¯æ´"
                
                summary = f"""{status_emoji} æ™‚åºæ¨¡å‹æ¨è«–å®Œæˆï¼

ğŸ¯ åˆ†æçµæœ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š é æ¸¬é¡åˆ¥: {result_name}
ğŸ“ˆ ä¿¡å¿ƒåº¦: {confidence:.3f} ({confidence*100:.1f}%)

ğŸ” è©³ç´°æ©Ÿç‡åˆ†ä½ˆ:
- ğŸ”¥ çœŸå¯¦ç«ç…™: {probabilities[0][1]:.3f} ({probabilities[0][1]*100:.1f}%)
- âŒ èª¤å ±äº‹ä»¶: {probabilities[0][0]:.3f} ({probabilities[0][0]*100:.1f}%)

âš™ï¸ è™•ç†åƒæ•¸:
- è¼¸å…¥å¹€æ•¸: {len(frames)} å¼µå½±åƒ
- æ™‚åºé•·åº¦: T=5 (å›ºå®šç­–ç•¥)
- æ¨¡å‹æ¶æ§‹: {self.current_model.backbone_name}
- èåˆæ–¹å¼: {self.current_model.temporal_fusion}

ğŸ’¾ çµæœæª”æ¡ˆ:
- è¦–è¦ºåŒ–çµæœ: {Path(output_path).name}
- å®Œæ•´è·¯å¾‘: {output_path}
{heatmap_info}

ğŸ”¬ è¦–è¦ºåŒ–èªªæ˜:
- Grad-CAM ç†±å€åœ–é¡¯ç¤ºæ¨¡å‹é—œæ³¨çš„å½±åƒå€åŸŸ
- ç´…è‰²å€åŸŸè¡¨ç¤ºé«˜é—œæ³¨åº¦ï¼Œè—è‰²å€åŸŸè¡¨ç¤ºä½é—œæ³¨åº¦
- çµ„åˆåœ–å±•ç¤ºåŸå§‹å¹€èˆ‡ç†±å€åœ–çš„å°æ¯”

ğŸ“… åˆ†ææ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                """
                
                self.inference_results = results
                return summary, results
                
            except Exception as e:
                results.append({
                    "sequence_id": "error_sequence",
                    "error": str(e)
                })
                return f"âŒ æ™‚åºæ¨è«–å¤±æ•—: {str(e)}", results
                
        except ImportError:
            return "âŒ ç¼ºå°‘æ™‚åºæ¨¡å‹ç›¸é—œä¾è³´", []
    
    def _generate_heatmaps(self, temporal_input, predicted_class):
        """ç”Ÿæˆç†±å€åœ–"""
        heatmaps = {}
        
        try:
            if self.gradcam_visualizer and TORCH_AVAILABLE:
                # ç”Ÿæˆ Grad-CAM
                cam = self.gradcam_visualizer.generate_cam(temporal_input, predicted_class)
                if cam is not None:
                    heatmaps['gradcam'] = cam
            
            if self.attention_visualizer:
                # ç”Ÿæˆæ³¨æ„åŠ›åœ°åœ–
                attention_maps = self.attention_visualizer.get_attention_maps()
                if attention_maps:
                    heatmaps['attention'] = attention_maps
        
        except Exception as e:
            print(f"ç”Ÿæˆç†±å€åœ–æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        return heatmaps
    
    def _create_heatmap_overlay(self, image, heatmap, alpha=0.4, colormap='jet'):
        """å°‡ç†±å€åœ–ç–ŠåŠ åˆ°åŸå§‹å½±åƒä¸Š"""
        try:
            if not MATPLOTLIB_AVAILABLE:
                return image
            
            # èª¿æ•´ç†±å€åœ–å¤§å°åˆ°å½±åƒå°ºå¯¸
            h, w = image.shape[:2]
            heatmap_resized = cv2.resize(heatmap, (w, h))
            
            # ä½¿ç”¨ matplotlib colormap
            cmap = cm.get_cmap(colormap)
            heatmap_colored = cmap(heatmap_resized)[:, :, :3]  # ç§»é™¤ alpha é€šé“
            heatmap_colored = (heatmap_colored * 255).astype(np.uint8)
            
            # å°‡ RGB è½‰ç‚º BGR (OpenCV æ ¼å¼)
            heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_RGB2BGR)
            
            # æ··åˆåŸå§‹å½±åƒå’Œç†±å€åœ–
            overlay = cv2.addWeighted(image, 1-alpha, heatmap_colored, alpha, 0)
            
            return overlay
            
        except Exception as e:
            print(f"å»ºç«‹ç†±å€åœ–ç–ŠåŠ æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return image
    
    def _save_heatmap_visualizations(self, frames, heatmaps, predicted_label, confidence, timestamp):
        """å„²å­˜ç†±å€åœ–è¦–è¦ºåŒ–çµæœ"""
        visualization_paths = []
        
        try:
            # å»ºç«‹ç†±å€åœ–ç›®éŒ„
            heatmap_dir = self.inference_dir / f"heatmaps_{timestamp}"
            heatmap_dir.mkdir(exist_ok=True)
            
            if 'gradcam' in heatmaps and len(frames) > 0:
                cam = heatmaps['gradcam']
                
                # ç‚ºæ¯å€‹å¹€ç”Ÿæˆç†±å€åœ–
                for i, frame in enumerate(frames[:5]):  # é™åˆ¶5å¹€
                    # å»ºç«‹ç–ŠåŠ åœ–
                    overlay = self._create_heatmap_overlay(frame, cam, alpha=0.4)
                    
                    # å„²å­˜å–®ç¨çš„ç†±å€åœ–
                    heatmap_path = heatmap_dir / f"gradcam_frame_{i+1}.jpg"
                    cv2.imwrite(str(heatmap_path), overlay)
                    visualization_paths.append(str(heatmap_path))
                
                # å»ºç«‹çµ„åˆè¦–è¦ºåŒ–
                combined_viz = self._create_combined_heatmap_visualization(
                    frames, cam, predicted_label, confidence, timestamp
                )
                combined_path = heatmap_dir / "combined_heatmap.jpg"
                cv2.imwrite(str(combined_path), combined_viz)
                visualization_paths.append(str(combined_path))
            
            return visualization_paths
            
        except Exception as e:
            print(f"å„²å­˜ç†±å€åœ–è¦–è¦ºåŒ–æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
    
    def _create_combined_heatmap_visualization(self, frames, heatmap, predicted_label, confidence, timestamp):
        """å»ºç«‹çµ„åˆçš„ç†±å€åœ–è¦–è¦ºåŒ–"""
        try:
            # é™åˆ¶é¡¯ç¤ºçš„å¹€æ•¸
            display_frames = frames[:3] if len(frames) > 3 else frames
            
            # èª¿æ•´æ¯å¹€å¤§å°
            target_size = (200, 200)
            
            # å»ºç«‹åŸå§‹å¹€å’Œç†±å€åœ–ç–ŠåŠ å¹€
            original_frames = []
            heatmap_frames = []
            
            for frame in display_frames:
                # åŸå§‹å¹€
                resized_original = cv2.resize(frame, target_size)
                original_frames.append(resized_original)
                
                # ç†±å€åœ–ç–ŠåŠ å¹€
                overlay = self._create_heatmap_overlay(resized_original, heatmap, alpha=0.5)
                heatmap_frames.append(overlay)
            
            # å»ºç«‹çµ„åˆç•«å¸ƒ
            padding = 15
            header_height = 80
            footer_height = 40
            
            canvas_width = target_size[0] * len(display_frames) * 2 + padding * (len(display_frames) * 2 + 1)
            canvas_height = target_size[1] + header_height + footer_height
            
            canvas = np.ones((canvas_height, canvas_width, 3), dtype=np.uint8) * 245
            
            # æ·»åŠ æ¨™é¡ŒèƒŒæ™¯
            cv2.rectangle(canvas, (0, 0), (canvas_width, header_height), (60, 60, 60), -1)
            
            # æ·»åŠ æ¨™é¡Œ
            title_text = "Grad-CAM Heatmap Visualization"
            cv2.putText(canvas, title_text, (20, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            
            # æ·»åŠ é æ¸¬çµæœ
            result_color = (0, 255, 0) if predicted_label == "true_positive" else (0, 165, 255)
            result_text = f"Prediction: {'Fire/Smoke' if predicted_label == 'true_positive' else 'No Fire/Smoke'}"
            confidence_text = f"Confidence: {confidence:.3f}"
            
            cv2.putText(canvas, result_text, (20, 55),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, result_color, 2)
            cv2.putText(canvas, confidence_text, (400, 55),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # æ”¾ç½®å¹€ - åŸå§‹å¹€å’Œç†±å€åœ–ä¸¦æ’
            y_start = header_height + 10
            
            for i in range(len(display_frames)):
                # åŸå§‹å¹€ä½ç½®
                orig_x = padding + i * (target_size[0] * 2 + padding)
                # ç†±å€åœ–ä½ç½®  
                heat_x = orig_x + target_size[0] + 5
                
                # æ”¾ç½®åŸå§‹å¹€
                canvas[y_start:y_start+target_size[1], orig_x:orig_x+target_size[0]] = original_frames[i]
                
                # æ”¾ç½®ç†±å€åœ–
                canvas[y_start:y_start+target_size[1], heat_x:heat_x+target_size[0]] = heatmap_frames[i]
                
                # æ·»åŠ æ¨™ç±¤
                cv2.putText(canvas, f"Original {i+1}", (orig_x, y_start-8),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (80, 80, 80), 1)
                cv2.putText(canvas, f"Grad-CAM {i+1}", (heat_x, y_start-8),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (80, 80, 80), 1)
                
                # æ·»åŠ ç™½è‰²é‚Šæ¡†
                cv2.rectangle(canvas, (orig_x-1, y_start-1), 
                             (orig_x+target_size[0]+1, y_start+target_size[1]+1), 
                             (255, 255, 255), 2)
                cv2.rectangle(canvas, (heat_x-1, y_start-1), 
                             (heat_x+target_size[0]+1, y_start+target_size[1]+1), 
                             (255, 255, 255), 2)
            
            # æ·»åŠ åº•éƒ¨èªªæ˜
            footer_y = y_start + target_size[1] + 15
            info_text = "Left: Original frames, Right: Grad-CAM heatmap overlay"
            cv2.putText(canvas, info_text, (20, footer_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 100), 1)
            
            return canvas
            
        except Exception as e:
            print(f"å»ºç«‹çµ„åˆç†±å€åœ–è¦–è¦ºåŒ–æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            # å›å‚³ç¬¬ä¸€å¹€ä½œç‚ºå‚™é¸
            return frames[0] if frames else np.zeros((224, 224, 3), dtype=np.uint8)
    
    
    def _create_temporal_result_grid(self, frames, predicted_label, confidence):
        """å»ºç«‹æ™‚åºçµæœç¶²æ ¼åœ–åƒ"""
        try:
            if not PIL_AVAILABLE:
                # å¦‚æœ PIL ä¸å¯ç”¨ï¼Œä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬
                return self._create_temporal_result_grid_english(frames, predicted_label, confidence)
            
            # é™åˆ¶é¡¯ç¤ºçš„å¹€æ•¸
            display_frames = frames[:5] if len(frames) > 5 else frames
            
            # èª¿æ•´æ¯å¹€å¤§å°
            target_size = (180, 180)
            resized_frames = []
            for frame in display_frames:
                resized = cv2.resize(frame, target_size)
                # å°‡ BGR è½‰æ›ç‚º RGB (PIL ä½¿ç”¨ RGB)
                resized_rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
                resized_frames.append(resized_rgb)
            
            # å»ºç«‹ç¶²æ ¼
            cols = len(resized_frames)
            padding = 10
            
            # å»ºç«‹ç©ºç™½ç•«å¸ƒ
            header_height = 80
            footer_height = 60
            canvas_height = target_size[1] + header_height + footer_height
            canvas_width = target_size[0] * cols + padding * (cols + 1)
            
            # ä½¿ç”¨ PIL å‰µå»ºåœ–åƒ
            canvas = Image.new('RGB', (canvas_width, canvas_height), color=(240, 240, 240))
            draw = ImageDraw.Draw(canvas)
            
            # å˜—è©¦è¼‰å…¥ä¸­æ–‡å­—é«”
            try:
                # å¸¸è¦‹çš„ä¸­æ–‡å­—é«”è·¯å¾‘
                font_paths = [
                    "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf",
                    "/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf",
                    "/System/Library/Fonts/Arial.ttf",  # macOS
                    "C:\\Windows\\Fonts\\Arial.ttf",    # Windows
                ]
                
                title_font = None
                text_font = None
                small_font = None
                
                for font_path in font_paths:
                    if Path(font_path).exists():
                        title_font = ImageFont.truetype(font_path, 24)
                        text_font = ImageFont.truetype(font_path, 18)
                        small_font = ImageFont.truetype(font_path, 14)
                        break
                
                # å¦‚æœæ‰¾ä¸åˆ°å­—é«”ï¼Œä½¿ç”¨é è¨­å­—é«”
                if not title_font:
                    title_font = ImageFont.load_default()
                    text_font = ImageFont.load_default()
                    small_font = ImageFont.load_default()
                    
            except Exception:
                title_font = ImageFont.load_default()
                text_font = ImageFont.load_default()
                small_font = ImageFont.load_default()
            
            # æ·»åŠ æ¨™é¡ŒèƒŒæ™¯
            draw.rectangle([(0, 0), (canvas_width, header_height)], fill=(50, 50, 50))
            
            # æ·»åŠ é æ¸¬çµæœæ¨™é¡Œ
            title_text = "Temporal Fire/Smoke Classification Result"
            draw.text((20, 15), title_text, fill=(255, 255, 255), font=title_font)
            
            # æ·»åŠ é æ¸¬çµæœ
            if predicted_label == "true_positive":
                result_text = "Prediction: Fire/Smoke Detected"
                result_color = (0, 255, 0)  # ç¶ è‰²
                status_text = "Fire/Smoke Event Detected"
            else:
                result_text = "Prediction: No Fire/Smoke"
                result_color = (255, 165, 0)  # æ©™è‰²
                status_text = "Non Fire/Smoke Event"
            
            confidence_text = f"Confidence: {confidence:.3f} ({confidence*100:.1f}%)"
            
            draw.text((20, 45), result_text, fill=result_color, font=text_font)
            draw.text((350, 45), confidence_text, fill=(255, 255, 255), font=text_font)
            
            # æ”¾ç½®å¹€
            for i, frame in enumerate(resized_frames):
                x_start = padding + i * (target_size[0] + padding)
                y_start = header_height + 10
                
                # æ·»åŠ ç™½è‰²é‚Šæ¡†
                border_box = [(x_start-2, y_start-2), (x_start+target_size[0]+2, y_start+target_size[1]+2)]
                draw.rectangle(border_box, outline=(255, 255, 255), width=2)
                
                # å°‡ numpy é™£åˆ—è½‰æ›ç‚º PIL åœ–åƒ
                frame_img = Image.fromarray(frame)
                canvas.paste(frame_img, (x_start, y_start))
                
                # æ·»åŠ å¹€ç·¨è™Ÿ
                frame_text = f"Frame {i+1}"
                draw.text((x_start + 5, y_start - 20), frame_text, fill=(100, 100, 100), font=small_font)
            
            # æ·»åŠ åº•éƒ¨ç‹€æ…‹
            footer_y = header_height + target_size[1] + 20
            footer_box = [(0, footer_y), (canvas_width, canvas_height)]
            draw.rectangle(footer_box, fill=(240, 240, 240))
            
            draw.text((20, footer_y + 15), status_text, fill=result_color, font=text_font)
            
            # æ·»åŠ è™•ç†ä¿¡æ¯
            info_text = f"Processed {len(display_frames)} frames, Temporal Length: T=5"
            draw.text((20, footer_y + 35), info_text, fill=(100, 100, 100), font=small_font)
            
            # å°‡ PIL åœ–åƒè½‰æ›å› OpenCV æ ¼å¼ (RGB -> BGR)
            canvas_array = np.array(canvas)
            canvas_bgr = cv2.cvtColor(canvas_array, cv2.COLOR_RGB2BGR)
            
            return canvas_bgr
            
        except Exception as e:
            print(f"å»ºç«‹çµæœç¶²æ ¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            # å›å‚³è‹±æ–‡ç‰ˆæœ¬ä½œç‚ºå‚™é¸
            return self._create_temporal_result_grid_english(frames, predicted_label, confidence)
    
    def _create_temporal_result_grid_english(self, frames, predicted_label, confidence):
        """å»ºç«‹æ™‚åºçµæœç¶²æ ¼åœ–åƒ (è‹±æ–‡ç‰ˆæœ¬)"""
        try:
            # é™åˆ¶é¡¯ç¤ºçš„å¹€æ•¸
            display_frames = frames[:5] if len(frames) > 5 else frames
            
            # èª¿æ•´æ¯å¹€å¤§å°
            target_size = (180, 180)
            resized_frames = []
            for frame in display_frames:
                resized = cv2.resize(frame, target_size)
                resized_frames.append(resized)
            
            # å»ºç«‹ç¶²æ ¼
            cols = len(resized_frames)
            padding = 10
            
            # å»ºç«‹ç©ºç™½ç•«å¸ƒ
            header_height = 80
            footer_height = 60
            canvas_height = target_size[1] + header_height + footer_height
            canvas_width = target_size[0] * cols + padding * (cols + 1)
            canvas = np.ones((canvas_height, canvas_width, 3), dtype=np.uint8) * 240
            
            # æ·»åŠ æ¨™é¡ŒèƒŒæ™¯
            cv2.rectangle(canvas, (0, 0), (canvas_width, header_height), (50, 50, 50), -1)
            
            # æ·»åŠ é æ¸¬çµæœæ¨™é¡Œ
            title_text = "Temporal Fire/Smoke Classification"
            cv2.putText(canvas, title_text, (20, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # æ·»åŠ é æ¸¬çµæœ
            if predicted_label == "true_positive":
                result_text = "Fire/Smoke Detected"
                result_color = (0, 255, 0)  # ç¶ è‰²
                status_text = "Fire/Smoke Event"
            else:
                result_text = "No Fire/Smoke"
                result_color = (0, 165, 255)  # æ©™è‰²
                status_text = "Non Fire/Smoke Event"
            
            confidence_text = f"Confidence: {confidence:.3f}"
            
            cv2.putText(canvas, result_text, (20, 55),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, result_color, 2)
            cv2.putText(canvas, confidence_text, (300, 55),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # æ”¾ç½®å¹€
            for i, frame in enumerate(resized_frames):
                x_start = padding + i * (target_size[0] + padding)
                y_start = header_height + 10
                
                # æ·»åŠ ç™½è‰²é‚Šæ¡†
                cv2.rectangle(canvas, (x_start-2, y_start-2), 
                             (x_start+target_size[0]+2, y_start+target_size[1]+2), 
                             (255, 255, 255), 2)
                
                canvas[y_start:y_start+target_size[1], x_start:x_start+target_size[0]] = frame
                
                # æ·»åŠ å¹€ç·¨è™Ÿ
                cv2.putText(canvas, f"Frame {i+1}", (x_start + 5, y_start - 8),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 100), 1)
            
            # æ·»åŠ åº•éƒ¨ç‹€æ…‹
            footer_y = header_height + target_size[1] + 20
            cv2.rectangle(canvas, (0, footer_y), (canvas_width, canvas_height), (240, 240, 240), -1)
            cv2.putText(canvas, status_text, (20, footer_y + 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, result_color, 2)
            
            # æ·»åŠ è™•ç†ä¿¡æ¯
            info_text = f"Processed {len(display_frames)} frames, T=5"
            cv2.putText(canvas, info_text, (20, footer_y + 50),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 100, 100), 1)
            
            return canvas
            
        except Exception as e:
            print(f"å»ºç«‹çµæœç¶²æ ¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            # å›å‚³ç¬¬ä¸€å¹€ä½œç‚ºå‚™é¸
            return frames[0] if frames else np.zeros((224, 224, 3), dtype=np.uint8)
    
    def get_inference_results_summary(self):
        """å–å¾—æ¨è«–çµæœæ‘˜è¦"""
        if not self.inference_results:
            return "å°šæœªé€²è¡Œæ¨è«–"
        
        successful_results = [r for r in self.inference_results if "error" not in r]
        error_results = [r for r in self.inference_results if "error" in r]
        
        summary_lines = [
            f"ğŸ“Š æ¨è«–çµæœæ‘˜è¦:",
            f"",
            f"âœ… æˆåŠŸè™•ç†: {len(successful_results)} å¼µå½±åƒ",
            f"âŒ è™•ç†å¤±æ•—: {len(error_results)} å¼µå½±åƒ",
            f""
        ]
        
        if successful_results:
            total_detections = sum(r["detection_count"] for r in successful_results)
            avg_detections = total_detections / len(successful_results)
            summary_lines.extend([
                f"ğŸ¯ ç¸½åµæ¸¬æ•¸: {total_detections} å€‹ç‰©ä»¶",
                f"ğŸ“ˆ å¹³å‡æ¯å¼µ: {avg_detections:.1f} å€‹ç‰©ä»¶",
                f""
            ])
            
            # é¡¯ç¤ºå„å½±åƒçš„çµæœ
            summary_lines.append("ğŸ“‹ è©³ç´°çµæœ:")
            for i, result in enumerate(successful_results[:10]):  # åªé¡¯ç¤ºå‰10å€‹
                filename = result["filename"][:30] + "..." if len(result["filename"]) > 30 else result["filename"]
                summary_lines.append(f"  {i+1}. {filename}: {result['detection_count']} å€‹ç‰©ä»¶")
            
            if len(successful_results) > 10:
                summary_lines.append(f"  ... é‚„æœ‰ {len(successful_results) - 10} å€‹çµæœ")
        
        if error_results:
            summary_lines.extend([
                f"",
                f"âŒ è™•ç†å¤±æ•—çš„æª”æ¡ˆ:",
            ])
            for result in error_results[:5]:  # åªé¡¯ç¤ºå‰5å€‹éŒ¯èª¤
                filename = result["filename"][:30] + "..." if len(result["filename"]) > 30 else result["filename"]
                error_msg = result["error"][:50] + "..." if len(result["error"]) > 50 else result["error"]
                summary_lines.append(f"  - {filename}: {error_msg}")
        
        return "\n".join(summary_lines)
    
    def get_detection_gallery(self):
        """å–å¾—åµæ¸¬çµæœå½±åƒç•«å»Š"""
        if not self.inference_results:
            return []
        
        gallery_paths = []
        for result in self.inference_results:
            # æª¢æŸ¥æ™‚åºæ¨¡å‹çµæœ
            if "result_image_path" in result and Path(result["result_image_path"]).exists():
                gallery_paths.append(result["result_image_path"])
            
            # æ·»åŠ ç†±å€åœ–è·¯å¾‘
            if "heatmap_paths" in result and result["heatmap_paths"]:
                for heatmap_path in result["heatmap_paths"]:
                    if Path(heatmap_path).exists():
                        gallery_paths.append(heatmap_path)
            
            # æª¢æŸ¥å…¶ä»–æ¨¡å‹çµæœ
            elif "annotated_image_path" in result and Path(result["annotated_image_path"]).exists():
                gallery_paths.append(result["annotated_image_path"])
        
        return gallery_paths
    
    def get_heatmap_gallery(self):
        """å–å¾—ç†±å€åœ–å°ˆç”¨ç•«å»Š"""
        if not self.inference_results:
            return []
        
        heatmap_paths = []
        for result in self.inference_results:
            if "heatmap_paths" in result and result["heatmap_paths"]:
                for path in result["heatmap_paths"]:
                    if Path(path).exists():
                        heatmap_paths.append(path)
        
        return heatmap_paths
    
    def clear_inference_results(self):
        """æ¸…é™¤æ¨è«–çµæœ"""
        self.inference_results = []
        
        # æ¸…é™¤æš«å­˜æª”æ¡ˆï¼ˆä¿ç•™æœ€è¿‘çš„çµæœï¼‰
        try:
            if self.inference_dir.exists():
                inference_files = list(self.inference_dir.glob("inference_result_*.jpg"))
                # ä¿ç•™æœ€æ–°çš„20å€‹æª”æ¡ˆï¼Œåˆªé™¤å…¶é¤˜çš„
                if len(inference_files) > 20:
                    inference_files.sort(key=lambda x: x.stat().st_mtime)
                    for old_file in inference_files[:-20]:
                        old_file.unlink()
        except Exception as e:
            print(f"æ¸…ç†æ¨è«–çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        return "âœ… å·²æ¸…é™¤æ¨è«–çµæœ"
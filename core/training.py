#!/usr/bin/env python3
"""
è¨“ç·´æ¨¡çµ„
è² è²¬æ¨¡å‹è¨“ç·´åŠŸèƒ½
"""

import os
import zipfile
from pathlib import Path
import json
import tempfile
import shutil
from datetime import datetime

try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False

try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False


class ModelTrainer:
    """æ¨¡å‹è¨“ç·´å™¨"""
    
    def __init__(self):
        self.training_dir = Path("training_workspace")
        self.training_dir.mkdir(exist_ok=True)
        
        # æ”¯æ´çš„æ¨¡å‹é¡å‹
        self.supported_models = {
            # YOLO æ¨¡å‹ï¼ˆç‰©ä»¶åµæ¸¬ï¼‰
            "yolov8n.pt": "YOLOv8 Nano - è¼•é‡ç´šç‰©ä»¶åµæ¸¬",
            "yolov8s.pt": "YOLOv8 Small - å¹³è¡¡æ€§èƒ½ç‰©ä»¶åµæ¸¬", 
            "yolov8m.pt": "YOLOv8 Medium - é«˜ç²¾åº¦ç‰©ä»¶åµæ¸¬",
            "yolov8l.pt": "YOLOv8 Large - æœ€é«˜ç²¾åº¦ç‰©ä»¶åµæ¸¬",
            "yolov8x.pt": "YOLOv8 Extra Large - æœ€é«˜ç²¾åº¦ç‰©ä»¶åµæ¸¬",
            
            # æ™‚åºåˆ†é¡æ¨¡å‹ï¼ˆåŸºæ–¼ timm backboneï¼‰
            "temporal_resnet50": "æ™‚åº ResNet50 + æ³¨æ„åŠ›æ©Ÿåˆ¶",
            "temporal_resnet18": "æ™‚åº ResNet18 + æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆè¼•é‡ï¼‰",
            "temporal_convnext_small": "æ™‚åº ConvNeXt Small + LSTM",
            "temporal_convnext_tiny": "æ™‚åº ConvNeXt Tiny + æ³¨æ„åŠ›æ©Ÿåˆ¶",
            "temporal_efficientnet_b3": "æ™‚åº EfficientNet B3 + æ³¨æ„åŠ›æ©Ÿåˆ¶",
            "temporal_efficientnet_b0": "æ™‚åº EfficientNet B0 + æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆè¼•é‡ï¼‰"
        }
        
        # è¨“ç·´ç‹€æ…‹
        self.is_training = False
        self.training_progress = ""
        self.training_results = None
    
    def upload_and_extract_dataset(self, zip_files):
        """ä¸Šå‚³ä¸¦è§£å£“å¤šå€‹æ¨™è¨»è³‡æ–™é›†"""
        try:
            if not zip_files:
                return "è«‹é¸æ“‡æ¨™è¨»è³‡æ–™é›†ZIPæª”æ¡ˆ"
            
            # ç¢ºä¿æ˜¯åˆ—è¡¨æ ¼å¼
            if not isinstance(zip_files, list):
                zip_files = [zip_files]
            
            # å»ºç«‹æ–°çš„åˆä½µè³‡æ–™é›†ç›®éŒ„
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            merged_dataset_dir = self.training_dir / f"merged_dataset_{timestamp}"
            merged_dataset_dir.mkdir(exist_ok=True)
            
            # å»ºç«‹åˆä½µç›®éŒ„çµæ§‹
            merged_true_positive_dir = merged_dataset_dir / "true_positive"
            merged_false_positive_dir = merged_dataset_dir / "false_positive"
            merged_true_positive_dir.mkdir(exist_ok=True)
            merged_false_positive_dir.mkdir(exist_ok=True)
            
            total_stats = {
                "true_positive": 0,
                "false_positive": 0,
                "total_images": 0,
                "processed_files": 0,
                "failed_files": []
            }
            
            # è™•ç†æ¯å€‹ZIPæª”æ¡ˆ
            for i, zip_file in enumerate(zip_files):
                try:
                    print(f"è™•ç†ç¬¬ {i+1}/{len(zip_files)} å€‹ZIPæª”æ¡ˆ: {Path(zip_file.name).name}")
                    
                    # å»ºç«‹è‡¨æ™‚è§£å£“ç›®éŒ„
                    temp_extract_dir = merged_dataset_dir / f"temp_extract_{i}"
                    temp_extract_dir.mkdir(exist_ok=True)
                    
                    # è§£å£“æª”æ¡ˆ
                    with zipfile.ZipFile(zip_file.name, 'r') as zip_ref:
                        zip_ref.extractall(temp_extract_dir)
                    
                    # é©—è­‰è³‡æ–™é›†çµæ§‹
                    validation_result = self._validate_dataset_structure(temp_extract_dir)
                    if not validation_result["valid"]:
                        total_stats["failed_files"].append({
                            "filename": Path(zip_file.name).name,
                            "error": validation_result["error"]
                        })
                        shutil.rmtree(temp_extract_dir)
                        continue
                    
                    # åˆä½µåˆ°ä¸»è³‡æ–™é›†
                    self._merge_dataset_to_main(temp_extract_dir, merged_dataset_dir, i)
                    
                    # ç´¯è¨ˆçµ±è¨ˆ
                    total_stats["true_positive"] += validation_result["stats"]["true_positive"]
                    total_stats["false_positive"] += validation_result["stats"]["false_positive"]
                    total_stats["total_images"] += validation_result["stats"]["total_images"]
                    total_stats["processed_files"] += 1
                    
                    # æ¸…ç†è‡¨æ™‚ç›®éŒ„
                    shutil.rmtree(temp_extract_dir)
                    
                except Exception as e:
                    total_stats["failed_files"].append({
                        "filename": Path(zip_file.name).name,
                        "error": str(e)
                    })
                    # æ¸…ç†å¯èƒ½çš„è‡¨æ™‚ç›®éŒ„
                    temp_dir = merged_dataset_dir / f"temp_extract_{i}"
                    if temp_dir.exists():
                        shutil.rmtree(temp_dir)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æˆåŠŸè™•ç†çš„æª”æ¡ˆ
            if total_stats["processed_files"] == 0:
                shutil.rmtree(merged_dataset_dir)
                error_details = "\n".join([f"- {f['filename']}: {f['error']}" for f in total_stats["failed_files"]])
                return f"âŒ æ‰€æœ‰æª”æ¡ˆè™•ç†å¤±æ•—:\n\n{error_details}"
            
            # è½‰æ›ç‚ºYOLOæ ¼å¼
            yolo_dataset_dir = self._convert_to_yolo_format(merged_dataset_dir, total_stats)
            
            # ç”Ÿæˆçµæœå ±å‘Š
            result_text = f"""âœ… å¤šè³‡æ–™é›†åˆä½µæˆåŠŸï¼
            
ğŸ“Š è™•ç†çµæœ:
- æˆåŠŸè™•ç†: {total_stats['processed_files']} å€‹ZIPæª”æ¡ˆ
- å¤±æ•—æª”æ¡ˆ: {len(total_stats['failed_files'])} å€‹

ğŸ“ˆ åˆä½µå¾Œçµ±è¨ˆ:
- çœŸå¯¦ç«ç…™äº‹ä»¶: {total_stats['true_positive']} å€‹
- èª¤åˆ¤äº‹ä»¶: {total_stats['false_positive']} å€‹  
- ç¸½å½±åƒæ•¸: {total_stats['total_images']} å¼µ

ğŸ“ YOLOè³‡æ–™é›†è·¯å¾‘: {yolo_dataset_dir}
            """
            
            # å¦‚æœæœ‰å¤±æ•—æª”æ¡ˆï¼Œæ·»åŠ è©³ç´°ä¿¡æ¯
            if total_stats["failed_files"]:
                result_text += "\n\nâš ï¸ å¤±æ•—æª”æ¡ˆè©³æƒ…:\n"
                for failed in total_stats["failed_files"]:
                    result_text += f"- {failed['filename']}: {failed['error']}\n"
            
            return result_text
            
        except Exception as e:
            return f"âŒ è³‡æ–™é›†è™•ç†å¤±æ•—: {str(e)}"
    
    def _validate_dataset_structure(self, dataset_dir):
        """é©—è­‰è³‡æ–™é›†çµæ§‹"""
        try:
            true_positive_dir = dataset_dir / "true_positive"
            false_positive_dir = dataset_dir / "false_positive"
            
            if not true_positive_dir.exists() or not false_positive_dir.exists():
                return {"valid": False, "error": "ç¼ºå°‘ true_positive æˆ– false_positive è³‡æ–™å¤¾"}
            
            # çµ±è¨ˆäº‹ä»¶å’Œå½±åƒæ•¸é‡
            true_events = list(true_positive_dir.iterdir()) if true_positive_dir.exists() else []
            false_events = list(false_positive_dir.iterdir()) if false_positive_dir.exists() else []
            
            total_images = 0
            for event_dir in true_events + false_events:
                if event_dir.is_dir():
                    images = list(event_dir.glob("*.jpg"))
                    total_images += len(images)
            
            stats = {
                "true_positive": len(true_events),
                "false_positive": len(false_events),
                "total_images": total_images
            }
            
            if total_images == 0:
                return {"valid": False, "error": "æœªæ‰¾åˆ°ä»»ä½•å½±åƒæª”æ¡ˆ"}
            
            return {"valid": True, "stats": stats}
            
        except Exception as e:
            return {"valid": False, "error": str(e)}
    
    def _merge_dataset_to_main(self, source_dataset_dir, target_dataset_dir, dataset_index):
        """å°‡å–®å€‹è³‡æ–™é›†åˆä½µåˆ°ä¸»è³‡æ–™é›†"""
        try:
            source_true_dir = source_dataset_dir / "true_positive"
            source_false_dir = source_dataset_dir / "false_positive"
            target_true_dir = target_dataset_dir / "true_positive"
            target_false_dir = target_dataset_dir / "false_positive"
            
            # åˆä½µ true_positive äº‹ä»¶
            if source_true_dir.exists():
                for event_dir in source_true_dir.iterdir():
                    if event_dir.is_dir():
                        # é‡å‘½åäº‹ä»¶ç›®éŒ„é¿å…è¡çª
                        new_event_name = f"dataset_{dataset_index}_{event_dir.name}"
                        target_event_dir = target_true_dir / new_event_name
                        shutil.copytree(event_dir, target_event_dir)
            
            # åˆä½µ false_positive äº‹ä»¶
            if source_false_dir.exists():
                for event_dir in source_false_dir.iterdir():
                    if event_dir.is_dir():
                        # é‡å‘½åäº‹ä»¶ç›®éŒ„é¿å…è¡çª
                        new_event_name = f"dataset_{dataset_index}_{event_dir.name}"
                        target_event_dir = target_false_dir / new_event_name
                        shutil.copytree(event_dir, target_event_dir)
                        
        except Exception as e:
            print(f"åˆä½µè³‡æ–™é›†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise e
    
    def _convert_to_yolo_format(self, dataset_dir, stats):
        """è½‰æ›ç‚ºYOLOè¨“ç·´æ ¼å¼"""
        # é€™è£¡å»ºç«‹åŸºæœ¬çš„YOLOè³‡æ–™é›†çµæ§‹
        # å¯¦éš›å¯¦ä½œæ™‚æœƒæ ¹æ“šå…·é«”éœ€æ±‚èª¿æ•´
        yolo_dir = dataset_dir.parent / f"yolo_{dataset_dir.name}"
        yolo_dir.mkdir(exist_ok=True)
        
        # å»ºç«‹åŸºæœ¬ç›®éŒ„çµæ§‹
        (yolo_dir / "images" / "train").mkdir(parents=True, exist_ok=True)
        (yolo_dir / "images" / "val").mkdir(parents=True, exist_ok=True)
        (yolo_dir / "labels" / "train").mkdir(parents=True, exist_ok=True)
        (yolo_dir / "labels" / "val").mkdir(parents=True, exist_ok=True)
        
        # å»ºç«‹dataset.yamlé…ç½®æª”
        dataset_config = {
            "train": str(yolo_dir / "images" / "train"),
            "val": str(yolo_dir / "images" / "val"),
            "nc": 2,  # é¡åˆ¥æ•¸é‡: ç«ç…™, ç„¡ç«ç…™
            "names": ["fire_smoke", "no_fire_smoke"]
        }
        
        with open(yolo_dir / "dataset.yaml", "w", encoding="utf-8") as f:
            if YAML_AVAILABLE:
                yaml.dump(dataset_config, f, default_flow_style=False, allow_unicode=True)
            else:
                # å¦‚æœæ²’æœ‰ yamlï¼Œæ‰‹å‹•å¯«å…¥é…ç½®
                f.write(f"train: {dataset_config['train']}\n")
                f.write(f"val: {dataset_config['val']}\n")
                f.write(f"nc: {dataset_config['nc']}\n")
                f.write(f"names: {dataset_config['names']}\n")
        
        return yolo_dir
    
    def get_available_models(self):
        """å–å¾—å¯ç”¨çš„é è¨“ç·´æ¨¡å‹åˆ—è¡¨"""
        return list(self.supported_models.keys())
    
    def get_model_info(self, model_name):
        """å–å¾—æ¨¡å‹è³‡è¨Š"""
        return self.supported_models.get(model_name, "æœªçŸ¥æ¨¡å‹")
    
    def start_training(self, dataset_path, model_name, epochs, batch_size, image_size):
        """é–‹å§‹è¨“ç·´æ¨¡å‹"""
        try:
            # æª¢æŸ¥æ˜¯å¦ç‚ºæ™‚åºæ¨¡å‹
            is_temporal_model = model_name.startswith("temporal_")
            
            if not is_temporal_model and not ULTRALYTICS_AVAILABLE:
                return "âŒ æœªå®‰è£ ultralytics å¥—ä»¶ï¼Œç„¡æ³•é€²è¡Œ YOLO è¨“ç·´"
            
            if self.is_training:
                return "âš ï¸ å·²æœ‰è¨“ç·´æ­£åœ¨é€²è¡Œä¸­"
            
            # é©—è­‰åƒæ•¸
            if not self._validate_dataset_path(dataset_path):
                return "âŒ è³‡æ–™é›†è·¯å¾‘ä¸å­˜åœ¨æˆ–æ ¼å¼éŒ¯èª¤"
            
            # è¨­å®šè¨“ç·´åƒæ•¸
            self.is_training = True
            self.training_progress = "ğŸš€ æº–å‚™é–‹å§‹è¨“ç·´..."
            
            if is_temporal_model:
                # æ™‚åºæ¨¡å‹è¨“ç·´
                return self._start_temporal_training(dataset_path, model_name, epochs, batch_size, image_size)
            else:
                # YOLO æ¨¡å‹è¨“ç·´
                return self._start_yolo_training(dataset_path, model_name, epochs, batch_size, image_size)
            
        except Exception as e:
            self.is_training = False
            return f"âŒ è¨“ç·´å•Ÿå‹•å¤±æ•—: {str(e)}"
    
    def _validate_dataset_path(self, dataset_path):
        """é©—è­‰è³‡æ–™é›†è·¯å¾‘"""
        if "è³‡æ–™é›†è·¯å¾‘:" not in str(dataset_path):
            return False
        # å¾çµæœæ–‡å­—ä¸­æå–å¯¦éš›è·¯å¾‘
        lines = str(dataset_path).split('\n')
        for line in lines:
            if "YOLOè³‡æ–™é›†è·¯å¾‘:" in line:
                actual_path = line.split("YOLOè³‡æ–™é›†è·¯å¾‘:")[-1].strip()
                return Path(actual_path).exists()
        return False
    
    def _start_temporal_training(self, dataset_path, model_name, epochs, batch_size, image_size):
        """é–‹å§‹æ™‚åºæ¨¡å‹è¨“ç·´"""
        try:
            # å°å…¥æ™‚åºæ¨¡å‹ç›¸é—œæ¨¡çµ„
            from .models.temporal_trainer import TemporalTrainer
            from .models.temporal_classifier import DEFAULT_MODEL_CONFIGS
            
            # æå–å¯¦éš›è³‡æ–™é›†è·¯å¾‘
            lines = str(dataset_path).split('\n')
            actual_dataset_path = None
            for line in lines:
                if "åˆä½µå¾Œçµ±è¨ˆ:" in line or "è³‡æ–™é›†è·¯å¾‘:" in line:
                    continue
                if "merged_dataset_" in line or "dataset_" in line:
                    actual_dataset_path = line.split(":")[-1].strip()
                    break
            
            if not actual_dataset_path:
                return "âŒ ç„¡æ³•è§£æè³‡æ–™é›†è·¯å¾‘"
            
            # å»ºç«‹æ¨¡å‹é…ç½®
            model_config = self._get_temporal_model_config(model_name, image_size)
            
            # å•Ÿå‹•è¨“ç·´ä»»å‹™ï¼ˆåœ¨èƒŒæ™¯åŸ·è¡Œï¼‰
            import threading
            def training_worker():
                try:
                    trainer = TemporalTrainer(model_config)
                    self.training_progress = "ğŸ”¥ æ­£åœ¨è¨“ç·´æ™‚åºæ¨¡å‹..."
                    
                    result = trainer.train(
                        dataset_path=actual_dataset_path,
                        epochs=epochs,
                        batch_size=batch_size,
                        learning_rate=1e-3,
                        val_split=0.2
                    )
                    
                    self.training_results = result
                    self.training_progress = f"âœ… æ™‚åºæ¨¡å‹è¨“ç·´å®Œæˆï¼æœ€ä½³æº–ç¢ºç‡: {result['best_val_accuracy']:.3f}"
                    self.is_training = False
                    
                except Exception as e:
                    self.training_progress = f"âŒ è¨“ç·´å¤±æ•—: {str(e)}"
                    self.is_training = False
            
            training_thread = threading.Thread(target=training_worker)
            training_thread.start()
            
            return f"""âœ… æ™‚åºæ¨¡å‹è¨“ç·´å·²å•Ÿå‹•ï¼
            
ğŸ¯ è¨“ç·´è¨­å®š:
- æ¨¡å‹é¡å‹: {model_name} (æ™‚åºåˆ†é¡)
- è³‡æ–™é›†: {actual_dataset_path}
- è¨“ç·´è¼ªæ•¸: {epochs}
- æ‰¹æ¬¡å¤§å°: {batch_size}
- å½±åƒå°ºå¯¸: {image_size}
- æ™‚åºå¹€æ•¸: 5 (å›ºå®š)

ğŸ”¥ ç‰¹è‰²åŠŸèƒ½:
- T=5 å›ºå®šå¹€è¼¸å…¥ç­–ç•¥
- ä¸è¶³5å¹€ï¼šé‡è¤‡å¡«å……+å¾®é‡å™ªéŸ³
- è¶…é5å¹€ï¼šç­‰è·å‡å‹»å–æ¨£
- timm backbone ç‰¹å¾µæå–
- æ³¨æ„åŠ›/LSTM æ™‚åºèåˆ

ğŸ“Š è«‹é—œæ³¨ä¸‹æ–¹é€²åº¦é¡¯ç¤º...
            """
            
        except ImportError:
            return "âŒ ç¼ºå°‘ timm æˆ–ç›¸é—œä¾è³´ï¼Œè«‹å®‰è£: pip install timm matplotlib seaborn"
        except Exception as e:
            return f"âŒ æ™‚åºæ¨¡å‹è¨“ç·´å•Ÿå‹•å¤±æ•—: {str(e)}"
    
    def _start_yolo_training(self, dataset_path, model_name, epochs, batch_size, image_size):
        """é–‹å§‹ YOLO æ¨¡å‹è¨“ç·´"""
        # åŸæœ‰çš„ YOLO è¨“ç·´é‚è¼¯
        return f"""âœ… YOLO è¨“ç·´ä»»å‹™å·²å•Ÿå‹•ï¼
        
ğŸ¯ è¨“ç·´è¨­å®š:
- è³‡æ–™é›†: {dataset_path}
- åŸºç¤æ¨¡å‹: {model_name}
- è¨“ç·´è¼ªæ•¸: {epochs}
- æ‰¹æ¬¡å¤§å°: {batch_size}
- å½±åƒå°ºå¯¸: {image_size}

ğŸ“Š è«‹é—œæ³¨ä¸‹æ–¹é€²åº¦é¡¯ç¤º...
        """
    
    def _get_temporal_model_config(self, model_name, image_size):
        """å–å¾—æ™‚åºæ¨¡å‹é…ç½®"""
        # å¾æ¨¡å‹åç¨±æå– backbone
        if "resnet50" in model_name:
            backbone = "resnet50"
            fusion = "attention"
        elif "resnet18" in model_name:
            backbone = "resnet18"
            fusion = "attention"
        elif "convnext_small" in model_name:
            backbone = "convnext_small"
            fusion = "lstm"
        elif "convnext_tiny" in model_name:
            backbone = "convnext_tiny"
            fusion = "attention"
        elif "efficientnet_b3" in model_name:
            backbone = "efficientnet_b3"
            fusion = "attention"
        elif "efficientnet_b0" in model_name:
            backbone = "efficientnet_b0"
            fusion = "attention"
        else:
            backbone = "resnet50"
            fusion = "attention"
        
        return {
            'backbone_name': backbone,
            'num_classes': 2,
            'temporal_frames': 5,
            'pretrained': True,
            'temporal_fusion': fusion,
            'dropout': 0.2,
            'freeze_backbone': False
        }
    
    def get_training_progress(self):
        """å–å¾—è¨“ç·´é€²åº¦"""
        if not self.is_training:
            return "ç­‰å¾…é–‹å§‹è¨“ç·´..."
        return self.training_progress
    
    def stop_training(self):
        """åœæ­¢è¨“ç·´"""
        if self.is_training:
            self.is_training = False
            self.training_progress = "â¹ï¸ è¨“ç·´å·²åœæ­¢"
            return "âœ… è¨“ç·´å·²åœæ­¢"
        return "âš ï¸ æ²’æœ‰æ­£åœ¨é€²è¡Œçš„è¨“ç·´"
    
    def list_trained_models(self):
        """åˆ—å‡ºå·²è¨“ç·´çš„æ¨¡å‹"""
        runs_dir = Path("runs/detect")
        if not runs_dir.exists():
            return []
        
        models = []
        for run_dir in runs_dir.iterdir():
            if run_dir.is_dir():
                weights_dir = run_dir / "weights"
                if weights_dir.exists():
                    best_pt = weights_dir / "best.pt"
                    if best_pt.exists():
                        models.append({
                            "name": run_dir.name,
                            "path": str(best_pt),
                            "created": best_pt.stat().st_mtime
                        })
        
        # æŒ‰å»ºç«‹æ™‚é–“æ’åº
        models.sort(key=lambda x: x["created"], reverse=True)
        return models
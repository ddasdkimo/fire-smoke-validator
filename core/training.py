#!/usr/bin/env python3
"""
è¨“ç·´æ¨¡çµ„
è² è²¬æ¨¡å‹è¨“ç·´åŠŸèƒ½
"""

import os
import zipfile
from pathlib import Path
import json
import tempfile
import shutil
from datetime import datetime

try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False

try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False


class ModelTrainer:
    """æ¨¡å‹è¨“ç·´å™¨"""
    
    def __init__(self):
        self.training_dir = Path("training_workspace")
        self.training_dir.mkdir(exist_ok=True)
        
        # æ”¯æ´çš„æ™‚åºåˆ†é¡æ¨¡å‹ï¼ˆæŒ‰ç…§ç”¨é€”åˆ†é¡ï¼‰
        self.supported_models = {
            # ä½å»¶é²ï¼ˆEdge/å³æ™‚ï¼‰- é©åˆé¡†ç²’å°ã€ROIå¤šã€GPUé ç®—ç·Šçš„å ´æ™¯
            "temporal_mobilenetv3_large": "ğŸš€ MobileNetV3-Large + æ³¨æ„åŠ› - ç§»å‹•ç«¯å„ªåŒ–ã€æ¥µä½å»¶é²",
            "temporal_ghostnet_100": "ğŸš€ GhostNet-100 + æ³¨æ„åŠ› - è¼•é‡é«˜æ•ˆã€åƒæ•¸å°‘",
            "temporal_repvgg_b0": "ğŸš€ RepVGG-B0 + æ³¨æ„åŠ› - æ¨è«–å„ªåŒ–ã€éƒ¨ç½²å‹å–„",
            "temporal_efficientnet_b0": "ğŸš€ EfficientNet-B0 + æ³¨æ„åŠ› - ç¶“å…¸è¼•é‡ã€ç©©å®š",
            "temporal_efficientnet_b1": "ğŸš€ EfficientNet-B1 + æ³¨æ„åŠ› - è¼•é‡å‡ç´šç‰ˆ",
            
            # å‡è¡¡ï¼ˆé€Ÿåº¦/æº–åº¦å¹³è¡¡ï¼Œå»ºè­°èµ·æ‰‹ï¼‰
            "temporal_convnext_tiny": "âš–ï¸ ConvNeXt-Tiny + æ³¨æ„åŠ› - å¸¸ç”¨ç©©å®šã€é¦–é¸å¹³è¡¡",
            "temporal_efficientnetv2_s": "âš–ï¸ EfficientNetV2-S + æ³¨æ„åŠ› - é€Ÿåº¦æº–åº¦å¹³è¡¡",
            "temporal_resnet50d": "âš–ï¸ ResNet50D + æ³¨æ„åŠ› - æ”¹è‰¯ç‰ˆResNetã€å¯é ",
            "temporal_resnet50": "âš–ï¸ ResNet50 + æ³¨æ„åŠ› - ç¶“å…¸éª¨å¹¹ã€å»£æ³›é©—è­‰",
            "temporal_regnety_032": "âš–ï¸ RegNetY-032 + æ³¨æ„åŠ› - é«˜æ•ˆç¶²è·¯æ¶æ§‹",
            
            # æº–åº¦å„ªå…ˆï¼ˆç®—åŠ›å¯æ¥å—ï¼‰- é©åˆé›¢ç·š/æº–å³æ™‚å ´æ™¯
            "temporal_convnext_base": "ğŸ¯ ConvNeXt-Base + LSTM - é«˜æº–åº¦ã€å¤§æ¨¡å‹",
            "temporal_efficientnetv2_m": "ğŸ¯ EfficientNetV2-M + LSTM - ä¸­å¤§å‹é«˜ç²¾åº¦",
            "temporal_swin_tiny": "ğŸ¯ Swin-Tiny + æ³¨æ„åŠ› - Transformerè¦–è¦ºæ¨¡å‹",
            "temporal_vit_small": "ğŸ¯ ViT-Small + æ³¨æ„åŠ› - ç´”Transformerï¼ˆéœ€å……è¶³è³‡æ–™ï¼‰"
        }
        
        # è¨“ç·´ç‹€æ…‹ - ä½¿ç”¨æŒä¹…åŒ–ç‹€æ…‹æ–‡ä»¶
        self.state_file = Path("training_workspace/training_state.json")
        self._load_training_state()
        
        # æª¢æŸ¥ä¸¦é‡ç½®åƒµæ­»çš„è¨“ç·´ç‹€æ…‹
        self._check_and_reset_training_state()
    
    def _load_training_state(self):
        """è¼‰å…¥æŒä¹…åŒ–çš„è¨“ç·´ç‹€æ…‹"""
        try:
            if self.state_file.exists():
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                self.is_training = state.get('is_training', False)
                self.training_progress = state.get('training_progress', '')
                self.training_results = state.get('training_results', None)
                print(f"âœ… [STATE] è¼‰å…¥è¨“ç·´ç‹€æ…‹: is_training={self.is_training}")
            else:
                # é è¨­ç‹€æ…‹
                self.is_training = False
                self.training_progress = ""
                self.training_results = None
                print("ğŸ”§ [STATE] ä½¿ç”¨é è¨­è¨“ç·´ç‹€æ…‹")
        except Exception as e:
            print(f"âš ï¸ [STATE] è¼‰å…¥ç‹€æ…‹å¤±æ•—: {e}")
            self.is_training = False
            self.training_progress = ""
            self.training_results = None
    
    def _save_training_state(self):
        """ä¿å­˜è¨“ç·´ç‹€æ…‹åˆ°æ–‡ä»¶"""
        try:
            state = {
                'is_training': self.is_training,
                'training_progress': self.training_progress,
                'training_results': self.training_results
            }
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ [STATE] ä¿å­˜è¨“ç·´ç‹€æ…‹: is_training={self.is_training}")
        except Exception as e:
            print(f"âš ï¸ [STATE] ä¿å­˜ç‹€æ…‹å¤±æ•—: {e}")
    
    def _check_and_reset_training_state(self):
        """æª¢æŸ¥ä¸¦é‡ç½®åƒµæ­»çš„è¨“ç·´ç‹€æ…‹"""
        try:
            import psutil
            import os
            
            # æª¢æŸ¥æ˜¯å¦æœ‰å¯¦éš›çš„ Python è¨“ç·´é€²ç¨‹åœ¨é‹è¡Œ
            training_process_found = False
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    if proc.info['name'] and 'python' in proc.info['name'].lower():
                        cmdline = proc.info['cmdline']
                        if cmdline and any('training' in str(cmd).lower() for cmd in cmdline):
                            training_process_found = True
                            print(f"ğŸ” [DEBUG-INIT] ç™¼ç¾è¨“ç·´é€²ç¨‹: PID {proc.info['pid']}")
                            break
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°å¯¦éš›çš„è¨“ç·´é€²ç¨‹ï¼Œé‡ç½®ç‹€æ…‹
            if not training_process_found and hasattr(self, 'is_training'):
                if self.is_training:
                    print("ğŸ”§ [DEBUG-INIT] é‡ç½®åƒµæ­»çš„è¨“ç·´ç‹€æ…‹")
                    self.is_training = False
                    self.training_progress = ""
                else:
                    print("âœ… [DEBUG-INIT] è¨“ç·´ç‹€æ…‹æ­£å¸¸")
            else:
                print(f"ğŸ” [DEBUG-INIT] è¨“ç·´é€²ç¨‹æª¢æŸ¥: æ‰¾åˆ°={training_process_found}")
                
        except ImportError:
            print("âš ï¸ [DEBUG-INIT] psutil ä¸å¯ç”¨ï¼Œè·³éé€²ç¨‹æª¢æŸ¥")
        except Exception as e:
            print(f"âš ï¸ [DEBUG-INIT] ç‹€æ…‹æª¢æŸ¥å¤±æ•—: {e}")
    
    def upload_and_extract_dataset(self, zip_files):
        """ä¸Šå‚³ä¸¦è§£å£“å¤šå€‹æ¨™è¨»è³‡æ–™é›†"""
        try:
            if not zip_files:
                return "è«‹é¸æ“‡æ¨™è¨»è³‡æ–™é›†ZIPæª”æ¡ˆ"
            
            # ç¢ºä¿æ˜¯åˆ—è¡¨æ ¼å¼
            if not isinstance(zip_files, list):
                zip_files = [zip_files]
            
            # å»ºç«‹æ–°çš„åˆä½µè³‡æ–™é›†ç›®éŒ„
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            merged_dataset_dir = self.training_dir / f"merged_dataset_{timestamp}"
            merged_dataset_dir.mkdir(exist_ok=True)
            
            # å»ºç«‹åˆä½µç›®éŒ„çµæ§‹
            merged_true_positive_dir = merged_dataset_dir / "true_positive"
            merged_false_positive_dir = merged_dataset_dir / "false_positive"
            merged_true_positive_dir.mkdir(exist_ok=True)
            merged_false_positive_dir.mkdir(exist_ok=True)
            
            total_stats = {
                "true_positive": 0,
                "false_positive": 0,
                "total_images": 0,
                "processed_files": 0,
                "failed_files": []
            }
            
            # è™•ç†æ¯å€‹ZIPæª”æ¡ˆ
            for i, zip_file in enumerate(zip_files):
                try:
                    print(f"è™•ç†ç¬¬ {i+1}/{len(zip_files)} å€‹ZIPæª”æ¡ˆ: {Path(zip_file.name).name}")
                    
                    # å»ºç«‹è‡¨æ™‚è§£å£“ç›®éŒ„
                    temp_extract_dir = merged_dataset_dir / f"temp_extract_{i}"
                    temp_extract_dir.mkdir(exist_ok=True)
                    
                    # è§£å£“æª”æ¡ˆ
                    with zipfile.ZipFile(zip_file.name, 'r') as zip_ref:
                        zip_ref.extractall(temp_extract_dir)
                    
                    # é©—è­‰è³‡æ–™é›†çµæ§‹
                    validation_result = self._validate_dataset_structure(temp_extract_dir)
                    if not validation_result["valid"]:
                        total_stats["failed_files"].append({
                            "filename": Path(zip_file.name).name,
                            "error": validation_result["error"]
                        })
                        shutil.rmtree(temp_extract_dir)
                        continue
                    
                    # åˆä½µåˆ°ä¸»è³‡æ–™é›†
                    self._merge_dataset_to_main(temp_extract_dir, merged_dataset_dir, i)
                    
                    # ç´¯è¨ˆçµ±è¨ˆ
                    total_stats["true_positive"] += validation_result["stats"]["true_positive"]
                    total_stats["false_positive"] += validation_result["stats"]["false_positive"]
                    total_stats["total_images"] += validation_result["stats"]["total_images"]
                    total_stats["processed_files"] += 1
                    
                    # æ¸…ç†è‡¨æ™‚ç›®éŒ„
                    shutil.rmtree(temp_extract_dir)
                    
                except Exception as e:
                    total_stats["failed_files"].append({
                        "filename": Path(zip_file.name).name,
                        "error": str(e)
                    })
                    # æ¸…ç†å¯èƒ½çš„è‡¨æ™‚ç›®éŒ„
                    temp_dir = merged_dataset_dir / f"temp_extract_{i}"
                    if temp_dir.exists():
                        shutil.rmtree(temp_dir)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æˆåŠŸè™•ç†çš„æª”æ¡ˆ
            if total_stats["processed_files"] == 0:
                shutil.rmtree(merged_dataset_dir)
                error_details = "\n".join([f"- {f['filename']}: {f['error']}" for f in total_stats["failed_files"]])
                return f"âŒ æ‰€æœ‰æª”æ¡ˆè™•ç†å¤±æ•—:\n\n{error_details}"
            
            # è½‰æ›ç‚ºæ™‚åºåˆ†é¡æ ¼å¼
            temporal_dataset_dir = self._convert_to_temporal_format(merged_dataset_dir, total_stats)
            
            # ç”Ÿæˆçµæœå ±å‘Š
            result_text = f"""âœ… å¤šè³‡æ–™é›†åˆä½µæˆåŠŸï¼
            
ğŸ“Š è™•ç†çµæœ:
- æˆåŠŸè™•ç†: {total_stats['processed_files']} å€‹ZIPæª”æ¡ˆ
- å¤±æ•—æª”æ¡ˆ: {len(total_stats['failed_files'])} å€‹

ğŸ“ˆ åˆä½µå¾Œçµ±è¨ˆ:
- çœŸå¯¦ç«ç…™äº‹ä»¶: {total_stats['true_positive']} å€‹
- èª¤åˆ¤äº‹ä»¶: {total_stats['false_positive']} å€‹  
- ç¸½å½±åƒæ•¸: {total_stats['total_images']} å¼µ

ğŸ“ æ™‚åºåˆ†é¡è³‡æ–™é›†è·¯å¾‘: {temporal_dataset_dir}
            """
            
            # å¦‚æœæœ‰å¤±æ•—æª”æ¡ˆï¼Œæ·»åŠ è©³ç´°ä¿¡æ¯
            if total_stats["failed_files"]:
                result_text += "\n\nâš ï¸ å¤±æ•—æª”æ¡ˆè©³æƒ…:\n"
                for failed in total_stats["failed_files"]:
                    result_text += f"- {failed['filename']}: {failed['error']}\n"
            
            return result_text
            
        except Exception as e:
            return f"âŒ è³‡æ–™é›†è™•ç†å¤±æ•—: {str(e)}"
    
    def _validate_dataset_structure(self, dataset_dir):
        """é©—è­‰è³‡æ–™é›†çµæ§‹"""
        try:
            true_positive_dir = dataset_dir / "true_positive"
            false_positive_dir = dataset_dir / "false_positive"
            
            if not true_positive_dir.exists() or not false_positive_dir.exists():
                return {"valid": False, "error": "ç¼ºå°‘ true_positive æˆ– false_positive è³‡æ–™å¤¾"}
            
            # çµ±è¨ˆäº‹ä»¶å’Œå½±åƒæ•¸é‡
            true_events = list(true_positive_dir.iterdir()) if true_positive_dir.exists() else []
            false_events = list(false_positive_dir.iterdir()) if false_positive_dir.exists() else []
            
            total_images = 0
            for event_dir in true_events + false_events:
                if event_dir.is_dir():
                    images = list(event_dir.glob("*.jpg"))
                    total_images += len(images)
            
            stats = {
                "true_positive": len(true_events),
                "false_positive": len(false_events),
                "total_images": total_images
            }
            
            if total_images == 0:
                return {"valid": False, "error": "æœªæ‰¾åˆ°ä»»ä½•å½±åƒæª”æ¡ˆ"}
            
            return {"valid": True, "stats": stats}
            
        except Exception as e:
            return {"valid": False, "error": str(e)}
    
    def _merge_dataset_to_main(self, source_dataset_dir, target_dataset_dir, dataset_index):
        """å°‡å–®å€‹è³‡æ–™é›†åˆä½µåˆ°ä¸»è³‡æ–™é›†"""
        try:
            source_true_dir = source_dataset_dir / "true_positive"
            source_false_dir = source_dataset_dir / "false_positive"
            target_true_dir = target_dataset_dir / "true_positive"
            target_false_dir = target_dataset_dir / "false_positive"
            
            # åˆä½µ true_positive äº‹ä»¶
            if source_true_dir.exists():
                for event_dir in source_true_dir.iterdir():
                    if event_dir.is_dir():
                        # é‡å‘½åäº‹ä»¶ç›®éŒ„é¿å…è¡çª
                        new_event_name = f"dataset_{dataset_index}_{event_dir.name}"
                        target_event_dir = target_true_dir / new_event_name
                        shutil.copytree(event_dir, target_event_dir)
            
            # åˆä½µ false_positive äº‹ä»¶
            if source_false_dir.exists():
                for event_dir in source_false_dir.iterdir():
                    if event_dir.is_dir():
                        # é‡å‘½åäº‹ä»¶ç›®éŒ„é¿å…è¡çª
                        new_event_name = f"dataset_{dataset_index}_{event_dir.name}"
                        target_event_dir = target_false_dir / new_event_name
                        shutil.copytree(event_dir, target_event_dir)
                        
        except Exception as e:
            print(f"åˆä½µè³‡æ–™é›†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise e
    
    def _convert_to_temporal_format(self, dataset_dir, stats):
        """è½‰æ›ç‚ºæ™‚åºåˆ†é¡è¨“ç·´æ ¼å¼"""
        # é€™è£¡å»ºç«‹åŸºæœ¬çš„æ™‚åºåˆ†é¡è³‡æ–™é›†çµæ§‹
        # å¯¦éš›å¯¦ä½œæ™‚æœƒæ ¹æ“šå…·é«”éœ€æ±‚èª¿æ•´
        temporal_dir = dataset_dir.parent / f"temporal_{dataset_dir.name}"
        temporal_dir.mkdir(exist_ok=True)
        
        # å»ºç«‹åŸºæœ¬ç›®éŒ„çµæ§‹
        (temporal_dir / "images" / "train").mkdir(parents=True, exist_ok=True)
        (temporal_dir / "images" / "val").mkdir(parents=True, exist_ok=True)
        (temporal_dir / "labels" / "train").mkdir(parents=True, exist_ok=True)
        (temporal_dir / "labels" / "val").mkdir(parents=True, exist_ok=True)
        
        # å»ºç«‹dataset.yamlé…ç½®æª”
        dataset_config = {
            "train": str(temporal_dir / "images" / "train"),
            "val": str(temporal_dir / "images" / "val"),
            "nc": 2,  # é¡åˆ¥æ•¸é‡: ç«ç…™, ç„¡ç«ç…™
            "names": ["fire_smoke", "no_fire_smoke"]
        }
        
        with open(temporal_dir / "dataset.yaml", "w", encoding="utf-8") as f:
            if YAML_AVAILABLE:
                yaml.dump(dataset_config, f, default_flow_style=False, allow_unicode=True)
            else:
                # å¦‚æœæ²’æœ‰ yamlï¼Œæ‰‹å‹•å¯«å…¥é…ç½®
                f.write(f"train: {dataset_config['train']}\n")
                f.write(f"val: {dataset_config['val']}\n")
                f.write(f"nc: {dataset_config['nc']}\n")
                f.write(f"names: {dataset_config['names']}\n")
        
        return temporal_dir
    
    def get_available_models(self):
        """å–å¾—å¯ç”¨çš„é è¨“ç·´æ¨¡å‹åˆ—è¡¨"""
        return list(self.supported_models.keys())
    
    def get_model_info(self, model_name):
        """å–å¾—æ¨¡å‹è³‡è¨Š"""
        base_info = self.supported_models.get(model_name, "æœªçŸ¥æ¨¡å‹")
        
        # ç²å–æ¨è–¦çš„è¼¸å…¥å°ºå¯¸
        temp_config = self._get_temporal_model_config(model_name, 0)  # å‚³å…¥0è®“å®ƒä½¿ç”¨é è¨­å€¼
        recommended_size = temp_config.get('default_input_size', 224)
        
        return f"{base_info}\nğŸ“ æ¨è–¦è¼¸å…¥å°ºå¯¸: {recommended_size}x{recommended_size}"
    
    def get_recommended_input_size(self, model_name):
        """å–å¾—æ¨¡å‹æ¨è–¦çš„è¼¸å…¥å°ºå¯¸"""
        temp_config = self._get_temporal_model_config(model_name, 0)
        return temp_config.get('default_input_size', 224)
    
    def update_model_selection(self, model_name):
        """ç•¶æ¨¡å‹é¸æ“‡æ”¹è®Šæ™‚æ›´æ–°è³‡è¨Šå’Œå»ºè­°å°ºå¯¸"""
        model_info = self.get_model_info(model_name)
        recommended_size = self.get_recommended_input_size(model_name)
        return model_info, recommended_size
    
    def start_training(self, dataset_path, model_name, epochs, batch_size, image_size):
        """é–‹å§‹æ™‚åºæ¨¡å‹è¨“ç·´"""
        try:
            print(f"ğŸ” [DEBUG-CORE] start_training è¢«èª¿ç”¨:")
            print(f"   dataset_path: {repr(dataset_path)}")
            print(f"   model_name: {model_name}")
            print(f"   is_training: {self.is_training}")
            
            if self.is_training:
                print("âš ï¸ [DEBUG-CORE] å·²æœ‰è¨“ç·´æ­£åœ¨é€²è¡Œä¸­")
                return "âš ï¸ å·²æœ‰è¨“ç·´æ­£åœ¨é€²è¡Œä¸­"
            
            # é©—è­‰åƒæ•¸
            print(f"ğŸ” [DEBUG-CORE] é–‹å§‹é©—è­‰è³‡æ–™é›†è·¯å¾‘...")
            path_valid = self._validate_dataset_path(dataset_path)
            print(f"ğŸ” [DEBUG-CORE] è·¯å¾‘é©—è­‰çµæœ: {path_valid}")
            
            if not path_valid:
                print("âŒ [DEBUG-CORE] è³‡æ–™é›†è·¯å¾‘é©—è­‰å¤±æ•—")
                return "âŒ è³‡æ–™é›†è·¯å¾‘ä¸å­˜åœ¨æˆ–æ ¼å¼éŒ¯èª¤"
            
            # è¨­å®šè¨“ç·´åƒæ•¸
            self.is_training = True
            self.training_progress = "ğŸš€ æº–å‚™é–‹å§‹è¨“ç·´..."
            self._save_training_state()  # ä¿å­˜ç‹€æ…‹
            
            # æ™‚åºæ¨¡å‹è¨“ç·´
            return self._start_temporal_training(dataset_path, model_name, epochs, batch_size, image_size)
            
        except Exception as e:
            self.is_training = False
            return f"âŒ è¨“ç·´å•Ÿå‹•å¤±æ•—: {str(e)}"
    
    def _validate_dataset_path(self, dataset_path):
        """é©—è­‰è³‡æ–™é›†è·¯å¾‘"""
        print(f"ğŸ” [DEBUG-VALIDATE] _validate_dataset_path è¢«èª¿ç”¨:")
        print(f"   dataset_path é¡å‹: {type(dataset_path)}")
        print(f"   dataset_path å…§å®¹: {repr(dataset_path)}")
        
        dataset_str = str(dataset_path)
        has_temporal = "æ™‚åºåˆ†é¡è³‡æ–™é›†è·¯å¾‘:" in dataset_str
        has_old = "è³‡æ–™é›†è·¯å¾‘:" in dataset_str
        
        print(f"ğŸ” [DEBUG-VALIDATE] å­—ä¸²æª¢æŸ¥:")
        print(f"   å«æœ‰ 'æ™‚åºåˆ†é¡è³‡æ–™é›†è·¯å¾‘:': {has_temporal}")
        print(f"   å«æœ‰ 'è³‡æ–™é›†è·¯å¾‘:': {has_old}")
        
        if not has_temporal and not has_old:
            print("âŒ [DEBUG-VALIDATE] æ²’æœ‰æ‰¾åˆ°ä»»ä½•è³‡æ–™é›†è·¯å¾‘æ¨™è¨˜")
            return False
        
        # å¾çµæœæ–‡å­—ä¸­æå–å¯¦éš›è·¯å¾‘
        lines = dataset_str.split('\n')
        print(f"ğŸ” [DEBUG-VALIDATE] åˆ†å‰²æˆ {len(lines)} è¡Œ")
        
        for i, line in enumerate(lines):
            print(f"   è¡Œ {i}: {repr(line)}")
            
            if "æ™‚åºåˆ†é¡è³‡æ–™é›†è·¯å¾‘:" in line:
                actual_path = line.split("æ™‚åºåˆ†é¡è³‡æ–™é›†è·¯å¾‘:")[-1].strip()
                print(f"ğŸ” [DEBUG-VALIDATE] æå–åˆ°è·¯å¾‘: {repr(actual_path)}")
                exists = Path(actual_path).exists()
                print(f"ğŸ” [DEBUG-VALIDATE] è·¯å¾‘å­˜åœ¨: {exists}")
                return exists
            elif "è³‡æ–™é›†è·¯å¾‘:" in line:
                actual_path = line.split("è³‡æ–™é›†è·¯å¾‘:")[-1].strip()
                print(f"ğŸ” [DEBUG-VALIDATE] æå–åˆ°è·¯å¾‘: {repr(actual_path)}")
                exists = Path(actual_path).exists()
                print(f"ğŸ” [DEBUG-VALIDATE] è·¯å¾‘å­˜åœ¨: {exists}")
                return exists
        
        print("âŒ [DEBUG-VALIDATE] æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è·¯å¾‘è¡Œ")
        return False
    
    def _start_temporal_training(self, dataset_path, model_name, epochs, batch_size, image_size):
        """é–‹å§‹æ™‚åºæ¨¡å‹è¨“ç·´"""
        try:
            # å°å…¥æ™‚åºæ¨¡å‹ç›¸é—œæ¨¡çµ„
            from .models.temporal_trainer import TemporalTrainer
            from .models.temporal_classifier import DEFAULT_MODEL_CONFIGS
            
            # æå–å¯¦éš›è³‡æ–™é›†è·¯å¾‘
            lines = str(dataset_path).split('\n')
            actual_dataset_path = None
            
            print(f"ğŸ” [DEBUG-TEMPORAL] é–‹å§‹è§£æè³‡æ–™é›†è·¯å¾‘")
            print(f"   ç¸½è¡Œæ•¸: {len(lines)}")
            
            for line in lines:
                print(f"   æª¢æŸ¥è¡Œ: {repr(line)}")
                
                # æª¢æŸ¥æ–°æ ¼å¼ï¼šæ™‚åºåˆ†é¡è³‡æ–™é›†è·¯å¾‘: xxx
                if "æ™‚åºåˆ†é¡è³‡æ–™é›†è·¯å¾‘:" in line:
                    temporal_path = line.split("æ™‚åºåˆ†é¡è³‡æ–™é›†è·¯å¾‘:")[-1].strip()
                    print(f"âœ… [DEBUG-TEMPORAL] æ‰¾åˆ°æ™‚åºè·¯å¾‘: {temporal_path}")
                    # å¾æ™‚åºè·¯å¾‘æ¨å°å‡ºåŸå§‹æ¨™è¨»è³‡æ–™é›†è·¯å¾‘
                    if temporal_path.startswith("training_workspace/temporal_"):
                        # temporal_merged_dataset_xxx -> merged_dataset_xxx
                        original_path = temporal_path.replace("temporal_", "")
                        print(f"ğŸ” [DEBUG-TEMPORAL] æ¨å°åŸå§‹è·¯å¾‘: {original_path}")
                        if Path(original_path).exists():
                            actual_dataset_path = original_path
                            print(f"âœ… [DEBUG-TEMPORAL] ä½¿ç”¨åŸå§‹æ¨™è¨»è³‡æ–™é›†: {actual_dataset_path}")
                            break
                    actual_dataset_path = temporal_path
                    break
                # æª¢æŸ¥èˆŠæ ¼å¼ï¼šè³‡æ–™é›†è·¯å¾‘: xxx (å¦‚æœæœ‰çš„è©±)
                elif "è³‡æ–™é›†è·¯å¾‘:" in line and "æ™‚åºåˆ†é¡" not in line:
                    # åªè™•ç†åŒ…å«å¯¦éš›è·¯å¾‘çš„è¡Œ
                    potential_path = line.split("è³‡æ–™é›†è·¯å¾‘:")[-1].strip()
                    if potential_path and "/" in potential_path:
                        actual_dataset_path = potential_path
                        print(f"âœ… [DEBUG-TEMPORAL] æ‰¾åˆ°èˆŠæ ¼å¼è·¯å¾‘: {actual_dataset_path}")
                        break
            
            if not actual_dataset_path:
                print(f"âŒ [DEBUG-TEMPORAL] ç„¡æ³•è§£æå‡ºæœ‰æ•ˆè·¯å¾‘")
                return "âŒ ç„¡æ³•è§£æè³‡æ–™é›†è·¯å¾‘"
            
            print(f"ğŸ¯ [DEBUG-TEMPORAL] æœ€çµ‚è·¯å¾‘: {actual_dataset_path}")
            
            # å»ºç«‹æ¨¡å‹é…ç½®
            model_config = self._get_temporal_model_config(model_name, image_size)
            
            # å•Ÿå‹•è¨“ç·´ä»»å‹™ï¼ˆåœ¨èƒŒæ™¯åŸ·è¡Œï¼‰
            import threading
            import os
            def training_worker():
                try:
                    print(f"ğŸš€ [TRAINING-WORKER] è¨“ç·´åŸ·è¡Œç·’å•Ÿå‹•")
                    print(f"   é€²ç¨‹ PID: {os.getpid()}")
                    print(f"   åŸ·è¡Œç·’ ID: {threading.current_thread().ident}")
                    print(f"   æ¨¡å‹é…ç½®: {model_config}")
                    
                    # å‰µå»ºè¨“ç·´å™¨
                    print(f"ğŸ—ï¸ [TRAINING-WORKER] æ­£åœ¨å‰µå»º TemporalTrainer...")
                    trainer = TemporalTrainer(model_config)
                    print(f"âœ… [TRAINING-WORKER] TemporalTrainer å‰µå»ºæˆåŠŸ")
                    
                    self.training_progress = "ğŸ”¥ æ­£åœ¨è¨“ç·´æ™‚åºæ¨¡å‹..."
                    print(f"ğŸ”¥ [TRAINING-WORKER] é–‹å§‹è¨“ç·´...")
                    print(f"   è³‡æ–™é›†è·¯å¾‘: {actual_dataset_path}")
                    print(f"   è¨“ç·´åƒæ•¸: epochs={epochs}, batch_size={batch_size}")
                    
                    result = trainer.train(
                        dataset_path=actual_dataset_path,
                        epochs=epochs,
                        batch_size=batch_size,
                        learning_rate=1e-3,
                        val_split=0.2
                    )
                    
                    print(f"âœ… [TRAINING-WORKER] è¨“ç·´å®Œæˆï¼çµæœ: {result}")
                    self.training_results = result
                    self.training_progress = f"âœ… æ™‚åºæ¨¡å‹è¨“ç·´å®Œæˆï¼æœ€ä½³æº–ç¢ºç‡: {result['best_val_accuracy']:.3f}"
                    self.is_training = False
                    self._save_training_state()  # ä¿å­˜æœ€çµ‚ç‹€æ…‹
                    
                except Exception as e:
                    print(f"âŒ [TRAINING-WORKER] è¨“ç·´å¤±æ•—: {str(e)}")
                    import traceback
                    print(f"ğŸ” [TRAINING-WORKER] éŒ¯èª¤å †ç–Š:")
                    traceback.print_exc()
                    self.training_progress = f"âŒ è¨“ç·´å¤±æ•—: {str(e)}"
                    self.is_training = False
                    self._save_training_state()  # ä¿å­˜éŒ¯èª¤ç‹€æ…‹
            
            training_thread = threading.Thread(target=training_worker)
            training_thread.daemon = False  # ç¢ºä¿åŸ·è¡Œç·’ä¸æœƒè¢«æå‰çµ‚æ­¢
            training_thread.start()
            print(f"ğŸš€ [TRAINING-MAIN] è¨“ç·´åŸ·è¡Œç·’å·²å•Ÿå‹•: {training_thread.ident}")
            
            return f"""âœ… æ™‚åºæ¨¡å‹è¨“ç·´å·²å•Ÿå‹•ï¼
            
ğŸ¯ è¨“ç·´è¨­å®š:
- æ¨¡å‹é¡å‹: {model_name} (æ™‚åºåˆ†é¡)
- è³‡æ–™é›†: {actual_dataset_path}
- è¨“ç·´è¼ªæ•¸: {epochs}
- æ‰¹æ¬¡å¤§å°: {batch_size}
- å½±åƒå°ºå¯¸: {image_size}
- æ™‚åºå¹€æ•¸: 5 (å›ºå®š)

ğŸ”¥ ç‰¹è‰²åŠŸèƒ½:
- T=5 å›ºå®šå¹€è¼¸å…¥ç­–ç•¥
- ä¸è¶³5å¹€ï¼šé‡è¤‡å¡«å……+å¾®é‡å™ªéŸ³
- è¶…é5å¹€ï¼šç­‰è·å‡å‹»å–æ¨£
- timm backbone ç‰¹å¾µæå–
- æ³¨æ„åŠ›/LSTM æ™‚åºèåˆ

ğŸ“Š è«‹é—œæ³¨ä¸‹æ–¹é€²åº¦é¡¯ç¤º...
            """
            
        except ImportError:
            return "âŒ ç¼ºå°‘ timm æˆ–ç›¸é—œä¾è³´ï¼Œè«‹å®‰è£: pip install timm matplotlib seaborn"
        except Exception as e:
            return f"âŒ æ™‚åºæ¨¡å‹è¨“ç·´å•Ÿå‹•å¤±æ•—: {str(e)}"
    
    
    def _get_temporal_model_config(self, model_name, image_size):
        """å–å¾—æ™‚åºæ¨¡å‹é…ç½®"""
        # å¾æ¨¡å‹åç¨±æå– backbone å’Œèåˆç­–ç•¥
        model_configs = {
            # ä½å»¶é²ï¼ˆEdge/å³æ™‚ï¼‰
            "temporal_mobilenetv3_large": ("mobilenetv3_large_100", "attention"),
            "temporal_ghostnet_100": ("ghostnet_100", "attention"),
            "temporal_repvgg_b0": ("repvgg_b0", "attention"),
            "temporal_efficientnet_b0": ("efficientnet_b0", "attention"),
            "temporal_efficientnet_b1": ("efficientnet_b1", "attention"),
            
            # å‡è¡¡ï¼ˆé€Ÿåº¦/æº–åº¦å¹³è¡¡ï¼‰
            "temporal_convnext_tiny": ("convnext_tiny", "attention"),
            "temporal_efficientnetv2_s": ("efficientnetv2_s", "attention"),
            "temporal_resnet50d": ("resnet50d", "attention"),
            "temporal_resnet50": ("resnet50", "attention"),
            "temporal_regnety_032": ("regnety_032", "attention"),
            
            # æº–åº¦å„ªå…ˆ
            "temporal_convnext_base": ("convnext_base", "lstm"),
            "temporal_efficientnetv2_m": ("efficientnetv2_m", "lstm"),
            "temporal_swin_tiny": ("swin_tiny_patch4_window7_224", "attention"),
            "temporal_vit_small": ("vit_small_patch16_224", "attention"),
        }
        
        # ç²å–é…ç½®ï¼Œé è¨­ä½¿ç”¨ convnext_tiny + attentionï¼ˆå‡è¡¡æ¨è–¦ï¼‰
        backbone, fusion = model_configs.get(model_name, ("convnext_tiny", "attention"))
        
        # æ ¹æ“šæ¨¡å‹é¡å‹èª¿æ•´ dropoutï¼ˆä½å»¶é²æ¨¡å‹ç”¨è¼ƒä½dropoutï¼Œå¤§æ¨¡å‹ç”¨è¼ƒé«˜ï¼‰
        dropout_configs = {
            # ä½å»¶é²æ¨¡å‹ - è¼ƒä½dropoutä»¥ä¿æŒè¼•é‡
            "mobilenetv3_large_100": 0.1,
            "ghostnet_100": 0.1,
            "repvgg_b0": 0.12,
            "efficientnet_b0": 0.12,
            "efficientnet_b1": 0.15,
            
            # å‡è¡¡æ¨¡å‹ - ä¸­ç­‰dropout
            "convnext_tiny": 0.15,
            "efficientnetv2_s": 0.18,
            "resnet50d": 0.2,
            "resnet50": 0.2,
            "regnety_032": 0.18,
            
            # æº–åº¦å„ªå…ˆæ¨¡å‹ - è¼ƒé«˜dropouté˜²éæ“¬åˆ
            "convnext_base": 0.3,
            "efficientnetv2_m": 0.25,
            "swin_tiny_patch4_window7_224": 0.2,
            "vit_small_patch16_224": 0.15,  # ViTé€šå¸¸è¼ƒæ•æ„Ÿ
        }
        
        dropout = dropout_configs.get(backbone, 0.18)  # é è¨­ç‚ºä¸­ç­‰å€¼
        
        # æ ¹æ“štimmæ¨¡å‹è¨­å®šå°æ‡‰çš„è¼¸å…¥åœ–ç‰‡å¤§å°
        input_size_configs = {
            # ä½å»¶é²æ¨¡å‹ - é€šå¸¸ä½¿ç”¨è¼ƒå°è¼¸å…¥å°ºå¯¸
            "mobilenetv3_large_100": 224,
            "ghostnet_100": 224,
            "repvgg_b0": 224,
            "efficientnet_b0": 224,
            "efficientnet_b1": 240,
            
            # å‡è¡¡æ¨¡å‹ - æ¨™æº–224æˆ–ç¨å¤§
            "convnext_tiny": 224,
            "efficientnetv2_s": 224,  # EfficientNetV2å¯ä»¥adaptive
            "resnet50d": 224,
            "resnet50": 224,
            "regnety_032": 224,
            
            # æº–åº¦å„ªå…ˆæ¨¡å‹ - è¼ƒå¤§è¼¸å…¥å°ºå¯¸
            "convnext_base": 224,  # ConvNeXtç³»åˆ—é€šå¸¸224
            "efficientnetv2_m": 288,  # EfficientNetV2-Må»ºè­°ä½¿ç”¨æ›´å¤§å°ºå¯¸
            "swin_tiny_patch4_window7_224": 224,  # Swinåç¨±ä¸­å°±æœ‰224
            "vit_small_patch16_224": 224,  # ViTåç¨±ä¸­å°±æœ‰224
        }
        
        # å–å¾—å°æ‡‰çš„è¼¸å…¥å°ºå¯¸ï¼Œå¦‚æœuseræœ‰æŒ‡å®šå‰‡ä½¿ç”¨userçš„è¨­å®š
        default_input_size = input_size_configs.get(backbone, 224)
        final_input_size = image_size if image_size and image_size > 0 else default_input_size
        
        return {
            'backbone_name': backbone,
            'num_classes': 2,
            'temporal_frames': 5,
            'pretrained': True,
            'temporal_fusion': fusion,
            'dropout': dropout,
            'freeze_backbone': False,
            'default_input_size': default_input_size  # ä¿å­˜é è¨­å€¼ä¾›åƒè€ƒ
        }
    
    def get_training_progress(self):
        """å–å¾—è¨“ç·´é€²åº¦"""
        # å¦‚æœè¨“ç·´å·²å®Œæˆä¸”æœ‰é€²åº¦ä¿¡æ¯ï¼Œé¡¯ç¤ºæœ€çµ‚çµæœ
        if not self.is_training and self.training_progress:
            return self.training_progress
        # å¦‚æœæ²’æœ‰é–‹å§‹è¨“ç·´ä¸”æ²’æœ‰é€²åº¦ä¿¡æ¯
        elif not self.is_training:
            return "ç­‰å¾…é–‹å§‹è¨“ç·´..."
        # å¦‚æœæ­£åœ¨è¨“ç·´
        return self.training_progress
    
    def stop_training(self):
        """åœæ­¢è¨“ç·´"""
        if self.is_training:
            self.is_training = False
            self.training_progress = "â¹ï¸ è¨“ç·´å·²åœæ­¢"
            self._save_training_state()  # ä¿å­˜åœæ­¢ç‹€æ…‹
            return "âœ… è¨“ç·´å·²åœæ­¢"
        return "âš ï¸ æ²’æœ‰æ­£åœ¨é€²è¡Œçš„è¨“ç·´"
    
    def list_trained_models(self):
        """åˆ—å‡ºå·²è¨“ç·´çš„æ¨¡å‹"""
        models = []
        
        # æª¢æŸ¥æ™‚åºåˆ†é¡æ¨¡å‹ (æ–°çš„ä¸»è¦æ¨¡å‹é¡å‹)
        temporal_runs_dir = Path("runs/temporal_training")
        if temporal_runs_dir.exists():
            for run_dir in temporal_runs_dir.iterdir():
                if run_dir.is_dir():
                    best_model = run_dir / "best_model.pth"
                    if best_model.exists():
                        models.append({
                            "name": f"æ™‚åºæ¨¡å‹ - {run_dir.name}",
                            "path": str(best_model),
                            "created": best_model.stat().st_mtime,
                            "type": "temporal"
                        })
        
        # æª¢æŸ¥ YOLO æ¨¡å‹ (å¦‚æœå­˜åœ¨)
        yolo_runs_dir = Path("runs/detect")
        if yolo_runs_dir.exists():
            for run_dir in yolo_runs_dir.iterdir():
                if run_dir.is_dir():
                    weights_dir = run_dir / "weights"
                    if weights_dir.exists():
                        best_pt = weights_dir / "best.pt"
                        if best_pt.exists():
                            models.append({
                                "name": f"YOLOæ¨¡å‹ - {run_dir.name}",
                                "path": str(best_pt),
                                "created": best_pt.stat().st_mtime,
                                "type": "yolo"
                            })
        
        # æŒ‰å»ºç«‹æ™‚é–“æ’åº
        models.sort(key=lambda x: x["created"], reverse=True)
        return models
    
    def delete_model(self, model_path):
        """åˆªé™¤æŒ‡å®šçš„æ¨¡å‹"""
        try:
            model_path = Path(model_path)
            if not model_path.exists():
                return f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_path}"
            
            # å¦‚æœæ˜¯æ™‚åºæ¨¡å‹ï¼Œåˆªé™¤æ•´å€‹ç›®éŒ„
            if model_path.suffix == '.pth':
                # åˆªé™¤æ•´å€‹è¨“ç·´ç›®éŒ„
                model_dir = model_path.parent
                if model_dir.name.startswith('temporal_'):
                    import shutil
                    shutil.rmtree(model_dir)
                    print(f"ğŸ—‘ï¸ å·²åˆªé™¤æ™‚åºæ¨¡å‹ç›®éŒ„: {model_dir}")
                    return f"âœ… å·²åˆªé™¤æ™‚åºæ¨¡å‹: {model_dir.name}"
                else:
                    # åªåˆªé™¤æ¨¡å‹æ–‡ä»¶
                    model_path.unlink()
                    return f"âœ… å·²åˆªé™¤æ¨¡å‹æ–‡ä»¶: {model_path.name}"
            
            # å¦‚æœæ˜¯ YOLO æ¨¡å‹
            elif model_path.suffix == '.pt':
                # åˆªé™¤æ•´å€‹ YOLO è¨“ç·´ç›®éŒ„
                weights_dir = model_path.parent
                if weights_dir.name == 'weights':
                    run_dir = weights_dir.parent
                    import shutil
                    shutil.rmtree(run_dir)
                    print(f"ğŸ—‘ï¸ å·²åˆªé™¤ YOLO æ¨¡å‹ç›®éŒ„: {run_dir}")
                    return f"âœ… å·²åˆªé™¤ YOLO æ¨¡å‹: {run_dir.name}"
                else:
                    model_path.unlink()
                    return f"âœ… å·²åˆªé™¤æ¨¡å‹æ–‡ä»¶: {model_path.name}"
            
            return f"âŒ ä¸æ”¯æ´çš„æ¨¡å‹æ ¼å¼: {model_path.suffix}"
            
        except Exception as e:
            return f"âŒ åˆªé™¤å¤±æ•—: {str(e)}"
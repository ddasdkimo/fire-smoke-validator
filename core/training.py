#!/usr/bin/env python3
"""
è¨“ç·´æ¨¡çµ„
è² è²¬æ¨¡å‹è¨“ç·´åŠŸèƒ½
"""

import os
import zipfile
from pathlib import Path
import json
import tempfile
import shutil
from datetime import datetime

try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False

try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False


class ModelTrainer:
    """æ¨¡å‹è¨“ç·´å™¨"""
    
    def __init__(self):
        self.training_dir = Path("training_workspace")
        self.training_dir.mkdir(exist_ok=True)
        
        # æ”¯æ´çš„æ¨¡å‹é¡å‹
        self.supported_models = {
            "yolov8n.pt": "YOLOv8 Nano - è¼•é‡ç´šï¼Œé€Ÿåº¦å¿«",
            "yolov8s.pt": "YOLOv8 Small - å¹³è¡¡æ€§èƒ½èˆ‡é€Ÿåº¦", 
            "yolov8m.pt": "YOLOv8 Medium - è¼ƒé«˜ç²¾åº¦",
            "yolov8l.pt": "YOLOv8 Large - é«˜ç²¾åº¦",
            "yolov8x.pt": "YOLOv8 Extra Large - æœ€é«˜ç²¾åº¦"
        }
        
        # è¨“ç·´ç‹€æ…‹
        self.is_training = False
        self.training_progress = ""
        self.training_results = None
    
    def upload_and_extract_dataset(self, zip_files):
        """ä¸Šå‚³ä¸¦è§£å£“å¤šå€‹æ¨™è¨»è³‡æ–™é›†"""
        try:
            if not zip_files:
                return "è«‹é¸æ“‡æ¨™è¨»è³‡æ–™é›†ZIPæª”æ¡ˆ"
            
            # ç¢ºä¿æ˜¯åˆ—è¡¨æ ¼å¼
            if not isinstance(zip_files, list):
                zip_files = [zip_files]
            
            # å»ºç«‹æ–°çš„åˆä½µè³‡æ–™é›†ç›®éŒ„
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            merged_dataset_dir = self.training_dir / f"merged_dataset_{timestamp}"
            merged_dataset_dir.mkdir(exist_ok=True)
            
            # å»ºç«‹åˆä½µç›®éŒ„çµæ§‹
            merged_true_positive_dir = merged_dataset_dir / "true_positive"
            merged_false_positive_dir = merged_dataset_dir / "false_positive"
            merged_true_positive_dir.mkdir(exist_ok=True)
            merged_false_positive_dir.mkdir(exist_ok=True)
            
            total_stats = {
                "true_positive": 0,
                "false_positive": 0,
                "total_images": 0,
                "processed_files": 0,
                "failed_files": []
            }
            
            # è™•ç†æ¯å€‹ZIPæª”æ¡ˆ
            for i, zip_file in enumerate(zip_files):
                try:
                    print(f"è™•ç†ç¬¬ {i+1}/{len(zip_files)} å€‹ZIPæª”æ¡ˆ: {Path(zip_file.name).name}")
                    
                    # å»ºç«‹è‡¨æ™‚è§£å£“ç›®éŒ„
                    temp_extract_dir = merged_dataset_dir / f"temp_extract_{i}"
                    temp_extract_dir.mkdir(exist_ok=True)
                    
                    # è§£å£“æª”æ¡ˆ
                    with zipfile.ZipFile(zip_file.name, 'r') as zip_ref:
                        zip_ref.extractall(temp_extract_dir)
                    
                    # é©—è­‰è³‡æ–™é›†çµæ§‹
                    validation_result = self._validate_dataset_structure(temp_extract_dir)
                    if not validation_result["valid"]:
                        total_stats["failed_files"].append({
                            "filename": Path(zip_file.name).name,
                            "error": validation_result["error"]
                        })
                        shutil.rmtree(temp_extract_dir)
                        continue
                    
                    # åˆä½µåˆ°ä¸»è³‡æ–™é›†
                    self._merge_dataset_to_main(temp_extract_dir, merged_dataset_dir, i)
                    
                    # ç´¯è¨ˆçµ±è¨ˆ
                    total_stats["true_positive"] += validation_result["stats"]["true_positive"]
                    total_stats["false_positive"] += validation_result["stats"]["false_positive"]
                    total_stats["total_images"] += validation_result["stats"]["total_images"]
                    total_stats["processed_files"] += 1
                    
                    # æ¸…ç†è‡¨æ™‚ç›®éŒ„
                    shutil.rmtree(temp_extract_dir)
                    
                except Exception as e:
                    total_stats["failed_files"].append({
                        "filename": Path(zip_file.name).name,
                        "error": str(e)
                    })
                    # æ¸…ç†å¯èƒ½çš„è‡¨æ™‚ç›®éŒ„
                    temp_dir = merged_dataset_dir / f"temp_extract_{i}"
                    if temp_dir.exists():
                        shutil.rmtree(temp_dir)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æˆåŠŸè™•ç†çš„æª”æ¡ˆ
            if total_stats["processed_files"] == 0:
                shutil.rmtree(merged_dataset_dir)
                error_details = "\n".join([f"- {f['filename']}: {f['error']}" for f in total_stats["failed_files"]])
                return f"âŒ æ‰€æœ‰æª”æ¡ˆè™•ç†å¤±æ•—:\n\n{error_details}"
            
            # è½‰æ›ç‚ºYOLOæ ¼å¼
            yolo_dataset_dir = self._convert_to_yolo_format(merged_dataset_dir, total_stats)
            
            # ç”Ÿæˆçµæœå ±å‘Š
            result_text = f"""âœ… å¤šè³‡æ–™é›†åˆä½µæˆåŠŸï¼
            
ğŸ“Š è™•ç†çµæœ:
- æˆåŠŸè™•ç†: {total_stats['processed_files']} å€‹ZIPæª”æ¡ˆ
- å¤±æ•—æª”æ¡ˆ: {len(total_stats['failed_files'])} å€‹

ğŸ“ˆ åˆä½µå¾Œçµ±è¨ˆ:
- çœŸå¯¦ç«ç…™äº‹ä»¶: {total_stats['true_positive']} å€‹
- èª¤åˆ¤äº‹ä»¶: {total_stats['false_positive']} å€‹  
- ç¸½å½±åƒæ•¸: {total_stats['total_images']} å¼µ

ğŸ“ YOLOè³‡æ–™é›†è·¯å¾‘: {yolo_dataset_dir}
            """
            
            # å¦‚æœæœ‰å¤±æ•—æª”æ¡ˆï¼Œæ·»åŠ è©³ç´°ä¿¡æ¯
            if total_stats["failed_files"]:
                result_text += "\n\nâš ï¸ å¤±æ•—æª”æ¡ˆè©³æƒ…:\n"
                for failed in total_stats["failed_files"]:
                    result_text += f"- {failed['filename']}: {failed['error']}\n"
            
            return result_text
            
        except Exception as e:
            return f"âŒ è³‡æ–™é›†è™•ç†å¤±æ•—: {str(e)}"
    
    def _validate_dataset_structure(self, dataset_dir):
        """é©—è­‰è³‡æ–™é›†çµæ§‹"""
        try:
            true_positive_dir = dataset_dir / "true_positive"
            false_positive_dir = dataset_dir / "false_positive"
            
            if not true_positive_dir.exists() or not false_positive_dir.exists():
                return {"valid": False, "error": "ç¼ºå°‘ true_positive æˆ– false_positive è³‡æ–™å¤¾"}
            
            # çµ±è¨ˆäº‹ä»¶å’Œå½±åƒæ•¸é‡
            true_events = list(true_positive_dir.iterdir()) if true_positive_dir.exists() else []
            false_events = list(false_positive_dir.iterdir()) if false_positive_dir.exists() else []
            
            total_images = 0
            for event_dir in true_events + false_events:
                if event_dir.is_dir():
                    images = list(event_dir.glob("*.jpg"))
                    total_images += len(images)
            
            stats = {
                "true_positive": len(true_events),
                "false_positive": len(false_events),
                "total_images": total_images
            }
            
            if total_images == 0:
                return {"valid": False, "error": "æœªæ‰¾åˆ°ä»»ä½•å½±åƒæª”æ¡ˆ"}
            
            return {"valid": True, "stats": stats}
            
        except Exception as e:
            return {"valid": False, "error": str(e)}
    
    def _merge_dataset_to_main(self, source_dataset_dir, target_dataset_dir, dataset_index):
        """å°‡å–®å€‹è³‡æ–™é›†åˆä½µåˆ°ä¸»è³‡æ–™é›†"""
        try:
            source_true_dir = source_dataset_dir / "true_positive"
            source_false_dir = source_dataset_dir / "false_positive"
            target_true_dir = target_dataset_dir / "true_positive"
            target_false_dir = target_dataset_dir / "false_positive"
            
            # åˆä½µ true_positive äº‹ä»¶
            if source_true_dir.exists():
                for event_dir in source_true_dir.iterdir():
                    if event_dir.is_dir():
                        # é‡å‘½åäº‹ä»¶ç›®éŒ„é¿å…è¡çª
                        new_event_name = f"dataset_{dataset_index}_{event_dir.name}"
                        target_event_dir = target_true_dir / new_event_name
                        shutil.copytree(event_dir, target_event_dir)
            
            # åˆä½µ false_positive äº‹ä»¶
            if source_false_dir.exists():
                for event_dir in source_false_dir.iterdir():
                    if event_dir.is_dir():
                        # é‡å‘½åäº‹ä»¶ç›®éŒ„é¿å…è¡çª
                        new_event_name = f"dataset_{dataset_index}_{event_dir.name}"
                        target_event_dir = target_false_dir / new_event_name
                        shutil.copytree(event_dir, target_event_dir)
                        
        except Exception as e:
            print(f"åˆä½µè³‡æ–™é›†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise e
    
    def _convert_to_yolo_format(self, dataset_dir, stats):
        """è½‰æ›ç‚ºYOLOè¨“ç·´æ ¼å¼"""
        # é€™è£¡å»ºç«‹åŸºæœ¬çš„YOLOè³‡æ–™é›†çµæ§‹
        # å¯¦éš›å¯¦ä½œæ™‚æœƒæ ¹æ“šå…·é«”éœ€æ±‚èª¿æ•´
        yolo_dir = dataset_dir.parent / f"yolo_{dataset_dir.name}"
        yolo_dir.mkdir(exist_ok=True)
        
        # å»ºç«‹åŸºæœ¬ç›®éŒ„çµæ§‹
        (yolo_dir / "images" / "train").mkdir(parents=True, exist_ok=True)
        (yolo_dir / "images" / "val").mkdir(parents=True, exist_ok=True)
        (yolo_dir / "labels" / "train").mkdir(parents=True, exist_ok=True)
        (yolo_dir / "labels" / "val").mkdir(parents=True, exist_ok=True)
        
        # å»ºç«‹dataset.yamlé…ç½®æª”
        dataset_config = {
            "train": str(yolo_dir / "images" / "train"),
            "val": str(yolo_dir / "images" / "val"),
            "nc": 2,  # é¡åˆ¥æ•¸é‡: ç«ç…™, ç„¡ç«ç…™
            "names": ["fire_smoke", "no_fire_smoke"]
        }
        
        with open(yolo_dir / "dataset.yaml", "w", encoding="utf-8") as f:
            if YAML_AVAILABLE:
                yaml.dump(dataset_config, f, default_flow_style=False, allow_unicode=True)
            else:
                # å¦‚æœæ²’æœ‰ yamlï¼Œæ‰‹å‹•å¯«å…¥é…ç½®
                f.write(f"train: {dataset_config['train']}\n")
                f.write(f"val: {dataset_config['val']}\n")
                f.write(f"nc: {dataset_config['nc']}\n")
                f.write(f"names: {dataset_config['names']}\n")
        
        return yolo_dir
    
    def get_available_models(self):
        """å–å¾—å¯ç”¨çš„é è¨“ç·´æ¨¡å‹åˆ—è¡¨"""
        return list(self.supported_models.keys())
    
    def get_model_info(self, model_name):
        """å–å¾—æ¨¡å‹è³‡è¨Š"""
        return self.supported_models.get(model_name, "æœªçŸ¥æ¨¡å‹")
    
    def start_training(self, dataset_path, model_name, epochs, batch_size, image_size):
        """é–‹å§‹è¨“ç·´æ¨¡å‹"""
        try:
            if not ULTRALYTICS_AVAILABLE:
                return "âŒ æœªå®‰è£ ultralytics å¥—ä»¶ï¼Œç„¡æ³•é€²è¡Œè¨“ç·´"
            
            if self.is_training:
                return "âš ï¸ å·²æœ‰è¨“ç·´æ­£åœ¨é€²è¡Œä¸­"
            
            # é©—è­‰åƒæ•¸
            if not Path(dataset_path).exists():
                return "âŒ è³‡æ–™é›†è·¯å¾‘ä¸å­˜åœ¨"
            
            # è¨­å®šè¨“ç·´åƒæ•¸
            self.is_training = True
            self.training_progress = "ğŸš€ æº–å‚™é–‹å§‹è¨“ç·´..."
            
            # å¯¦éš›è¨“ç·´é‚è¼¯æœƒåœ¨é€™è£¡å¯¦ä½œ
            # é€™è£¡å…ˆå»ºç«‹æ¡†æ¶
            
            return f"""âœ… è¨“ç·´ä»»å‹™å·²å•Ÿå‹•ï¼
            
ğŸ¯ è¨“ç·´è¨­å®š:
- è³‡æ–™é›†: {dataset_path}
- åŸºç¤æ¨¡å‹: {model_name}
- è¨“ç·´è¼ªæ•¸: {epochs}
- æ‰¹æ¬¡å¤§å°: {batch_size}
- å½±åƒå°ºå¯¸: {image_size}

ğŸ“Š è«‹é—œæ³¨ä¸‹æ–¹é€²åº¦é¡¯ç¤º...
            """
            
        except Exception as e:
            self.is_training = False
            return f"âŒ è¨“ç·´å•Ÿå‹•å¤±æ•—: {str(e)}"
    
    def get_training_progress(self):
        """å–å¾—è¨“ç·´é€²åº¦"""
        if not self.is_training:
            return "ç­‰å¾…é–‹å§‹è¨“ç·´..."
        return self.training_progress
    
    def stop_training(self):
        """åœæ­¢è¨“ç·´"""
        if self.is_training:
            self.is_training = False
            self.training_progress = "â¹ï¸ è¨“ç·´å·²åœæ­¢"
            return "âœ… è¨“ç·´å·²åœæ­¢"
        return "âš ï¸ æ²’æœ‰æ­£åœ¨é€²è¡Œçš„è¨“ç·´"
    
    def list_trained_models(self):
        """åˆ—å‡ºå·²è¨“ç·´çš„æ¨¡å‹"""
        runs_dir = Path("runs/detect")
        if not runs_dir.exists():
            return []
        
        models = []
        for run_dir in runs_dir.iterdir():
            if run_dir.is_dir():
                weights_dir = run_dir / "weights"
                if weights_dir.exists():
                    best_pt = weights_dir / "best.pt"
                    if best_pt.exists():
                        models.append({
                            "name": run_dir.name,
                            "path": str(best_pt),
                            "created": best_pt.stat().st_mtime
                        })
        
        # æŒ‰å»ºç«‹æ™‚é–“æ’åº
        models.sort(key=lambda x: x["created"], reverse=True)
        return models